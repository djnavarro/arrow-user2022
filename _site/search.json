[
  {
    "objectID": "hello-arrow.html",
    "href": "hello-arrow.html",
    "title": "Part 1: Hello Arrow",
    "section": "",
    "text": "Hello, and welcome to the Apache Arrow workshop! We hope you find it valuable.\nIf you’ve arrived on this page then presumably you’re an R user with some familiarity with tidyverse (especially dplyr) workflows, and would like to learn more about how to work with big data sets using the arrow package for R. Our goal in this workshop is to help you get there!\nIn this tutorial you will learn how to use the arrow R package to create seamless engineering-to-analysis data pipelines. You’ll learn how to use the parquet file format for efficient storage and data access. You’ll learn how to exercise fine control over data types to avoid common data pipeline problems. During the tutorial you’ll be processing larger-than-memory files and multi-file datasets with familiar dplyr syntax, and working with data in cloud storage. You’ll learn how the Apache Arrow project is structured and what you can do with Arrow to handle very large data sets. With any luck, you’ll have some fun too!\nHopefully, you’ve installed the packages you’ll need and have a copy of the downloaded onto your local machine. If not, it’s probably a good idea to check out the packages and data page before continuing :)"
  },
  {
    "objectID": "hello-arrow.html#get-started",
    "href": "hello-arrow.html#get-started",
    "title": "Part 1: Hello Arrow",
    "section": "Let’s get started!",
    "text": "Let’s get started!\nIf you’re going to go through this workshop, we ought to be able to give you a decent motivation for doing so, right? There’s no point in you learning about the arrow package if we can’t explain why it’s a good thing to know! But as all good writers know, sometimes it’s more powerful to show rather than tell, so let’s just start using arrow now and hold the explanations to the end.\nLet’s start by loading arrow and dplyr\n\nlibrary(arrow)\nlibrary(dplyr)\n\nActually, you can assume that every page on this site starts with those two packages and all the other dependencies loaded, even if this code chunk isn’t shown explicitly:\n\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(duckdb)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(palmerpenguins)\nlibrary(tictoc)\nlibrary(scales)\nlibrary(janitor)\nlibrary(fs)\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(sf)\n\nWhen you do this yourself you’ll see some additional messages that I’ve hidden from the output. Now that we have the packages loaded, we have access to the dplyr functions for data manipulation and (importantly!) the arrow back end that allows you to use dplyr syntax to manipulate data sets that are stored in Arrow.\nLet’s see this in action. I’ll use the open_dataset() function to connect to the NYC taxi data. On my machine this data is stored in the ~/Datasets/nyc-taxi folder, so my code looks like this:\n\nnyc_taxi <- open_dataset(\"~/Datasets/nyc-taxi\")\n\nYours will look slightly different depending on where you saved your local copy of the data. Now, if you remember from when you downloaded the data this is 69GB of information, yet open_dataset() returns a result almost immediately. What’s happened here is that arrow has scanned the contents of that folder, found the relevant files, and created an nyc_taxi object that stores some metadata about those files. Here’s what that looks like:\n\nnyc_taxi\n\nFileSystemDataset with 158 Parquet files\nvendor_name: string\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\ntrip_distance: double\npickup_longitude: double\npickup_latitude: double\nrate_code: string\nstore_and_fwd: string\ndropoff_longitude: double\ndropoff_latitude: double\npayment_type: string\nfare_amount: double\nextra: double\nmta_tax: double\ntip_amount: double\ntolls_amount: double\ntotal_amount: double\nimprovement_surcharge: double\ncongestion_surcharge: double\npickup_location_id: int64\ndropoff_location_id: int64\nyear: int32\nmonth: int32\n\n\nFor an R user used to working with data frames and tibbles, this output is likely to be unfamiliar. Each variable is listed as a row in the output, and next to the name of each variable you can see the type of data stored in it. It does represent a tabular data set just like a data frame, but it’s a different kind of thing. It has to be: behind the scenes there are 1.7 billion rows of data in one huge table, and this is just too big to load into memory. However, we can still work with it anyway. Here’s a simple example: I’ll extract the first six rows using the head() function and then collect() those rows from Arrow and return them to R:\n\nnyc_taxi |>\n  head() |>\n  collect()\n\n# A tibble: 6 × 24\n  vendor_name pickup_datetime     dropoff_datetime    passenger_count\n  <chr>       <dttm>              <dttm>                        <int>\n1 VTS         2009-01-04 13:52:00 2009-01-04 14:02:00               1\n2 VTS         2009-01-04 14:31:00 2009-01-04 14:38:00               3\n3 VTS         2009-01-04 02:43:00 2009-01-04 02:57:00               5\n4 DDS         2009-01-02 07:52:58 2009-01-02 08:14:00               1\n5 DDS         2009-01-25 03:18:23 2009-01-25 03:24:56               1\n6 DDS         2009-01-17 09:35:59 2009-01-17 09:43:35               2\n# … with 20 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <int>,\n#   dropoff_location_id <int>, year <int>, month <int>\n\n\nThe output here is a perfectly ordinary tibble object that now exists in R just like any other object, and – in case you haven’t already looked at it – we have posted a data dictionary explaining what each of these columns represents. Of course, this version of the table only has a handful of rows, but if you imagine this table stretching out for a lot more you have a good sense of what the actual nyc_taxi data looks like. How many rows?\n\nnrow(nyc_taxi)\n\n[1] 1672590319\n\n\nYes, this really is a table with 1.7 billion rows. Yikes.\nOkay so let’s do something with it. Suppose I wanted to look at the five-year trends for the number of taxi rides in NYC, both in total and specifically for shared trips that have multiple passengers. I can do this using dplyr commands like this:\n\nuse filter() to limit the data to the period from 2017 to 2021\nuse group_by() to set year as the grouping variable\nuse summarize() to count the number of all_trips and shared_trips\nuse mutate() to compute a pct_shared column with the percent of trips shared\nuse collect() to trigger execution of the query and return results to R\n\nHere’s what that looks like:\n\nnyc_taxi |>\n  filter(year %in% 2017:2021) |> \n  group_by(year) |>\n  summarize(\n    all_trips = n(),\n    shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n  ) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect()\n\n# A tibble: 5 × 4\n   year all_trips shared_trips pct_shared\n  <int>     <int>        <int>      <dbl>\n1  2017 113495512     32296166       28.5\n2  2018 102797401     28796633       28.0\n3  2019  84393604     23515989       27.9\n4  2020  24647055      5837960       23.7\n5  2021  30902618      7221844       23.4\n\n\nTry typing this out yourself and then have a go at the exercises!\n\n\n\n\n\n\nExercises\n\n\n\n\nProblemsSolution 1Solution 2\n\n\n\nCalculate the total number of rides for every month in 2019\nFor each month in 2019, find the distance travelled by the longest recorded taxi ride that month and sort the results in month order\n\n\n\n\nnyc_taxi |> \n  filter(year == 2019) |>\n  count(month) |>\n  collect()\n\n# A tibble: 12 × 2\n   month       n\n   <int>   <int>\n 1     1 7667255\n 2    10 7213588\n 3    12 6895933\n 4    11 6877463\n 5     3 7832035\n 6     4 7432826\n 7     5 7564884\n 8     2 7018750\n 9     6 6940489\n10     7 6310134\n11     8 6072851\n12     9 6567396\n\n\n\n\n\nnyc_taxi |> \n  filter(year == 2019) |>\n  group_by(month) |>\n  summarize(longest_trip = max(trip_distance, na.rm = TRUE)) |>\n  arrange(month) |> \n  collect()\n\n# A tibble: 12 × 2\n   month longest_trip\n   <int>        <dbl>\n 1     1         832.\n 2     2         702.\n 3     3         237.\n 4     4         831.\n 5     5         401.\n 6     6       45977.\n 7     7         312.\n 8     8         602.\n 9     9         604.\n10    10         308.\n11    11         701.\n12    12       19130.\n\n\nAnd there you have it! Your first data analysis using arrow is done. Yay!"
  },
  {
    "objectID": "hello-arrow.html#what-is-arrow",
    "href": "hello-arrow.html#what-is-arrow",
    "title": "Part 1: Hello Arrow",
    "section": "What is Arrow and why should you care?",
    "text": "What is Arrow and why should you care?\nOkay, let’s take a moment to unpack this. We just analyzed 1.7 billion rows of data in R, a language that has not traditionally been thought to be ideal for big data! How did we do that? Clearly the arrow package is doing some magic, but what is that magic and how does it work?\nTo demystify what just happened here we need talk about what the Apache Arrow project is all about and why it’s exciting. So let’s start at the beginning. There’s a one sentence description of Arrow that I really like, and has an almost haiku-like quality to it\n\nA multi-language toolbox  for accelerated data interchange  and in-memory processing\n\nThis description immediately tells you three important things:\n\nArrow is a single project that has implementations or bindings in many different programming languages: whether you use R, Python, C++, Ruby, Rust, Julia, JavaScript, or a variety of other languages, you can use Arrow.\nArrow makes it easier and faster (sometimes shockingly faster!) to share data among different applications and programming languages\nArrow formats exist to structure data efficiently in-memory (as opposed to on-disk), and provides compute engines to allow you process those data\n\nLet’s unpack these three things.\n\nArrow as a multi-language toolbox\nAt an abstract level, Arrow provides a collection of specifications for how tabular data should be stored in-memory, and how that data should be communicated when transferred from one process to another. That’s the top level in the diagram below:\n\n\n\n\n\nThere are several independent implementations of the Arrow standards, shown in the middle layer in the diagram. At present there are libraries in C++, C#, Go, Java, JavaScript, Julia and Rust, so anyone who uses one of those languages can store, manipulate, and communicate Arrow-formatted data sets. Because these are independent libraries, they are routinely tested against one another to ensure they produce the same behaviour!\nFor other programming languages there isn’t a native Arrow implementation, and instead the libraries supply bindings to the Arrow C++ library. That’s what happens for R, Python, Ruby, and MATLAB, and is shown in the bottom layer of the diagram above. If you’re using one of these languages, you can still work with Arrow data using these bindings. In R, for example, those bindings are supplied by the arrow package: it allows R users to write native R code that is translated to instructions that get executed by the Arrow C++ library.\n\n\n\n\n\n\nThe Arrow documentation\n\n\n\nBecause Arrow is a multi-language project, the documentation ends up spread across several places. R users new to the project will be most interested in the Apache Arrow R cookbook and the arrow package website. When you’re ready to look at the detail, it’s helpful to read a little on the Arrow specification. If you’re interested in contributing to the project the new contributor’s guide has information relevant for R and other languages.\n\n\n\n\nArrow for accelerated data interchange\nOne huge benefit to Arrow as a coherent multi-language toolbox is that it creates the possibility of using Arrow as a kind of “lingua franca” for data analysis, and that has huge benefits for allowing efficient communication. Here’s what we mean. The diagram below shows a situation where we have three applications, one written in R, another in Python, and a third one in C++.\n\n\n\n\n\nSuppose we have a data frame in-memory in the R application, and we want to communicate that with the Python application. Python doesn’t natively use the same in-memory data structures as R: the Python app might be expecting to use pandas instead. So the data has to be converted from one in-memory format (data frame) to another (panda). If the data set is large, this conversion cost could be quite computationally expensive.\nBut it’s worse than that too: the R and Python apps need some way to communicate. They don’t actually share the same memory, and they probably aren’t running on the same machine. So the apps would also need to agree on a communication standard: the data frame (in R memory) needs to be encoded – or “serialized”, to use the precise term – to a byte stream and transmitted as a message that can be decoded – or “deserialzed” – by Python at the other end and then reorganised into a panda data structure.\nAll of that is extremely inefficient. When you add costs associated with serializing data to and from on-disk file storage (e.g., as a CSV file or a parquet file) you start to realise that all these “copy and convert” operations are using up a lot of your compute time!\nNow think about what happens if all three applications rely on an Arrow library. Once loaded, the data itself stays in memory allocated to Arrow, and the only thing that the applications have to do is pass pointers around! This is shown below:\n\n\n\n\n\nBetter yet, if you ever need to transfer this data from one machine to another (e.g., if the applications are running on different machines and need local copies of the data), the Arrow specification tells you exactly what format to use to send the data, and – importantly – that format is designed so the “thing” you send over the communication channel has the exact same structure as the “thing” that has to be represented in memory. In other words, you don’t have to reorganised the message from the serialized format to something you can analyze: it’s already structured the right way!\nBut wait, it gets even better! (Ugh, yes, we know this does sound like one of those tacky infomercials that promise you free steak knives if you call in the next five minutes, but it actually does get better…) Because there are all these Arrow libraries that exist in different languages, you don’t have to do much of the coding yourself. The “connecters” that link one system to another are already written for you.\nNeat, yes?\n\n\nArrow for efficient in-memory processing\nSo far we’ve talked about the value of Arrow in terms of its multi-lingual nature and ability to serve as an agreed-upon standard for data representation and transfer. These are huge benefits in themselves, but that’s a generic argument about the importance of standardization and providing libraries for as many languages as you can. It doesn’t necessarily mean that the Arrow specifications ought to be the ones we agree to. So is there a reason why we should adopt this specific tool as the standard? Well yes, there is: Arrow is designed for efficient data operations.\nHere’s what we mean. Let’s take this small table as our working example, containing four rows and three columns:\n\n\n\n\n\nAlthough the data are rectangular in shape, a tabular data set has a fundamental asymmetry: rows refer to cases, and columns refer to variables. This almost always has the consequence that data stored in the same column are fundamentally similar. The years 1969 and 1959 are both integer valued (same type) and have related semantics (refer to a year-long period of time). What that means is that any operation you might want to do to 1969 (e.g., check if it was a leap year), you are probably likely to want to do the same operation with 1959. From a data analysis point of view, these are very likely to be processed using the same instructions.\nThe same doesn’t apply when it comes to rows. Although the year 1969 and the city \"New York\" are both properties associated with the Stonewall Inn riots, they have fundamentally different structure (one is represented as an integer, and the other as a string), and they have different semantics. You might want to check if 1969 was a leap year, but it makes no sense whatsoever to ask if New York city is a leap year, does it? In other words, from a data analysis perspective, we necessarily use different instructions to work with these two things.\nOkay, so now think about what happens when we want to store this table in memory. Memory addresses aren’t organised into two dimensional tables, they’re arranged as a sequential address space. That means we have to flatten this table into a one-dimensional structure. We can either do this row-wise, or we can do it column-wise. Here’s what that looks like:\n\n\n\n\n\nOn the left you can see what happens when cells are arranged row-wise. Adjacent cells aren’t very similar to each other. On the right, you can see what happens when cells are arranged column-wise (often referred to as a columnar format): adjacent items in memory tend to have the same type, and correspond to the same ontological category (e.g., cities are next to other cities, intra-city locations are next to other such locations).\nYes, but why does this matter? It matters because modern processors have a feature that let you take this advantage of this kind of memory locality, known as “single instruction, multiple data” (SIMD). Using SIMD programming, it’s possible to send a single low level instruction to the CPU, and have the CPU execute it across an entire block of memory without needing to send separate instructions for each memory address. In some ways it’s similar to the ideas behind parallel computing by multithreading, but it applies at a much lower level!\nSo as long as your data are organised in a columnar format (which Arrow data are) and your compute engine understands how to take advantage of modern CPU features like SIMD (which the Arrow compute engine does), you can speed up your data analysis by a substantial amount!"
  },
  {
    "objectID": "hello-arrow.html#arrow-for-r",
    "href": "hello-arrow.html#arrow-for-r",
    "title": "Part 1: Hello Arrow",
    "section": "What can the arrow package do for R users?",
    "text": "What can the arrow package do for R users?\nBroadly speaking we can divide the core functionality of the arrow package into two types of operation: Reading and writing data, and manipulating data\n\nRead/write capabilities\nThe diagram below shows (most of) the read/write capabilities of arrow in a schematic fashion:\n\n\n\n\n\nThere’s several things to comment on here.\nFirst, let’s look at the “destinations” column on the right hand side. You can read and write files (top right row) to your local storage, or you can send them to an S3 bucket. Regardless of where you’re reading from or writing to, arrow supports CSV (and other delimited text files), parquet, and feather formats. Not shown in the diagram are JSON files: arrow can read data from JSON format but not write to it.\nSecond, let’s look at the “objects” column on the left hand side. This column shows the three main data objects you’re likely to work with using arrow: data frames (and tibbles), Arrow Tables, and Arrow Datasets. Data frames are regular R objects stored in R memory, and Tables are the analogous data structure stored in Arrow memory. In contrast, Arrow Datasets are more complicated objects used when the data are too large to fit in memory, and typically stored on disk as multiple files. You can read and write any of these objects to and from disk using arrow. (Note that there are other data structures used by arrow that aren’t shown here!)\nThird, let’s look at the “stream” row on the bottom right. Instead of writing data to file, the arrow package allows you to stream it to a remote process. You can do this with in-memory data objects like Tables and data frames, but not for Datasets. Although not shown on the diagram, you can also do this for Arrow Record Batches.\nFinally, there is one big omission from this table: Arrow Flight servers. The arrow package contains functionality supporting Arrow Flight, but that’s not covered in this workshop!\n\n\nData manipulation\nThe data manipulation abilities of the arrow package come in a few different forms. For instance, one thing that arrow does is provide a low-level interface to the Arrow C++ library: you can call the compute functions in the C++ library directly from R if you want. However, for most R users this isn’t what you really want to do, because the Arrow C++ library has its own syntax and doesn’t feel very “R-like”.\nTo address this, arrow also supplies a dplyr backend, allowing R users to write normal dplyr code for data wrangling, and translating it into a form that the Arrow C++ library can execute. In that sense it’s very similar to the dbplyr package that does the same thing for SQL database queries:\n\n\n\n\n\nIt’s this dplyr back end that we relied on in the demonstration at the start of the session."
  },
  {
    "objectID": "slides.html#housekeeping",
    "href": "slides.html#housekeeping",
    "title": "Larger-Than-Memory Data Workflows with Apache Arrow",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nWebsite: arrow-user2022.netlify.app\n\nWritten tutorial, data sets, exercises\n\nInstructors:\n\nDanielle Navarro\nJonathan Keane\nStephanie Hazlitt"
  },
  {
    "objectID": "slides.html#structure",
    "href": "slides.html#structure",
    "title": "Larger-Than-Memory Data Workflows with Apache Arrow",
    "section": "Structure",
    "text": "Structure\n\nHello Arrow\nData Wrangling\nData Storage\nAdvanced Arrow"
  },
  {
    "objectID": "packages-and-data.html",
    "href": "packages-and-data.html",
    "title": "Packages and Data",
    "section": "",
    "text": "Welcome to the workshop! On this page you’ll find information about the packages and data sets we’ll be using. The workshop proper starts on the Hello Arrow page, but you may find this page useful to read before the workshop begins."
  },
  {
    "objectID": "packages-and-data.html#packages",
    "href": "packages-and-data.html#packages",
    "title": "Packages and Data",
    "section": "Packages",
    "text": "Packages\nTo install the required packages, run the following:\n\ninstall.packages(c(\n  \"arrow\", \"dplyr\", \"dbplyr\", \"duckdb\", \"fs\", \"janitor\",\n  \"palmerpenguins\", \"remotes\", \"scales\", \"stringr\", \n  \"lubridate\", \"tictoc\"\n))\n\nTo load them:\n\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(duckdb)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(palmerpenguins)\nlibrary(tictoc)\nlibrary(scales)\nlibrary(janitor)\nlibrary(fs)\n\nThe workshop doesn’t use graphics or spatial data much but they are used on this page and again at the end. Suggested packages if you want to run those parts of the code can be installed with the following:\n\ninstall.packages(c(\"ggplot2\", \"ggrepel\", \"sf\"))\n\nLoad the packages with this:\n\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(sf)"
  },
  {
    "objectID": "packages-and-data.html#data",
    "href": "packages-and-data.html#data",
    "title": "Packages and Data",
    "section": "Data",
    "text": "Data\nThroughout the workshop we’ll be relying on the New York City taxicab data set (instructions on obtaining the data are included below). In its full form, the data set takes the form of one very large table with about 1.7 billion rows and 24 columns. Each row corresponds to a single taxi ride sometime between 2009 and 2022. The contents of the columns are as follows:\n\nvendor_name (string): A code indicating the service provider that maintains the taxicab technology system in that vehicle. Possible values here are “VTS”, “DDS” and “CMT”.\npickup_datetime (timestamp): Specifies the time at which the customer was picked up. This is stored as an Arrow timestamp, which is analogous to a POSIXct object in R and can be manipulated in similar ways.\ndropoff_datetime (timestamp): Specifies the time at which the customer was dropped off.\npassenger_count (int64): An integer specifying the number of passengers. Note that this is not automatically computed: the value is manually entered by the driver.\ntrip_distance (double): The elapsed trip distance reported by the taximeter. Note that this is a US based dataset so the distance is recorded in miles (not kilometers).\npickup_longitude (double): Longitude data for the pickup location\npickup_latitude (double): Latitude data for the pickup location\nrate_code (string): Different trips charge fees at different rates, often due to airport surcharges, and those rates are assigned codes. String specifying the final rate code in effect at the end of the trip. Possible values are: “Standard rate”, “JFK”, “Newark”, “Negotiated”, “Nassau or Westchester”, and “Group ride”\nstore_and_fwd (string): Sometimes the vehicle does not have an internet connection and so the trip data is not immediately transmitted to the server. This is referred to as a “store and forward” case. The store_and_fwd flag is set to “Yes” when the vehicle did not immediately transmit, and “No” when it was able to transmit immediately.\ndropoff_longitude (double): Longitude data for the dropoff location\ndropoff_latitude (double): Latitude data for the dropoff location\npayment_type (string): Variable indicating how the customer paid for the ride. Values that occur in the data are “Cash”, “Credit card”, “No charge”, “Dispute” and “Unknown” (and of course missing data), but “Voided trip” is also possible according to the original documentation.\nfare_amount (double): The “time and distance” fare in US dollars calculated by the meter.\nextra (double): Additional extras and surcharges added to the fare amount. This includes US$0.50 and US$1 rush hour and overnight charges.\nmta_tax (double): The MTA is the “Metropolitan Transportation Authority” that governs public transport in New York City. This variable includes the US$0.50 MTA tax that is triggered based on the rate code in use.\ntip_amount (double): In the US it is customary for passengers to provide the taxi driver with an additional “tip” payment for service. This field records the tip amount paid only for credit card transactions: cash tips are not recorded.\ntolls_amount (double): As in many cities, various roads in NYC charge additional fees (“tolls”) to use them. Any tolls incurred are recorded here.\ntotal_amount (double): This is the total amount charged to the customer (excluding cash tips, which are not recorded in the data set).\n\nimprovement_surcharge (double): The improvement surcharge is an additional US$0.30 fee that began in 2015. According to the official documentation it is “assessed on hailed trips at the flag drop”. I honestly have no idea what that means.\n\ncongestion_surcharge (double): As in many cities, NYC imposes a congestion surcharge for vehicles that enter certain locations in the city. Any congestion fee incurred is recorded here.\npickup_location_id (int64): The “TLC Taxi Zone” in which the pickup occurred. This is a numeric id ranging from 1 to 265: more information about the taxi zones is provided below.\ndropoff_location_id (int64): The “TLC Taxi Zone” in which the dropoff occurred.\nyear (int32): The year in which the ride took place\nmonth (int32): The month in which the ride took place\n\n\nThe tiny NYC taxi data set\nIn a moment we’ll talk about how to get the full data set, but let’s start with a simpler version!\nIn practice, not everyone has time or hard disk space to download the entire data set. On top of that, it’s not always a good idea to do your learning on an enormous data set: mistakes become more time consuming with big data. So it’s often helpful to practice with a smaller data set that has the exact same structure as the larger one that you want to use later. To help out with that, we’ve created the “Tiny NYC Taxi” data that contains only 1 in 1000 rows from the original data set. So instead of working with 1.7 billion rows of data and about 70GB of files, the tiny taxi data set is 1.7 million rows and about 80MB of files.\nAll you have to do is download the nyc-taxi-tiny.zip archive and unzip it, and you’re ready to start. On my machine I saved a copy of the data to a folder called \"~/Datasets/nyc-taxi-tiny\". So if I wanted to use the tiny taxi data for this workshop, I could open the data set with this command:\n\nnyc_taxi <- open_dataset(\"~/Datasets/nyc-taxi-tiny\")\n\n\n\nThe full NYC taxi data set\nObtaining a copy of the full data set requires a little bit more effort. It’s stored online in an Amazon S3 bucket, and you can download all the files directly from there. In fact, the arrow package has commands to do that. On my machine I saved the full data set to a folder called \"~/Datasets/nyc-taxi\", so the command I used looked like this:\n\ncopy_files(\n  from = s3_bucket(\"ursa-labs-taxi-data-v2\"),\n  to = \"~/Datasets/nyc-taxi\"\n)\n\nBe warned!\nConceptually this is easy, but in practice it may be painful. Depending on the quality of your internet connection this is likely to take a long time. The data set is 69GB in size. The data set spans the years 2009 through 2022 and contains one parquet file per month. Here’s what you should expect to see for a single year:\n\nlist.files(\"~/Datasets/nyc-taxi/year=2018\", recursive = TRUE)\n\n [1] \"month=1/part-0.parquet\"  \"month=10/part-0.parquet\"\n [3] \"month=11/part-0.parquet\" \"month=12/part-0.parquet\"\n [5] \"month=2/part-0.parquet\"  \"month=3/part-0.parquet\" \n [7] \"month=4/part-0.parquet\"  \"month=5/part-0.parquet\" \n [9] \"month=6/part-0.parquet\"  \"month=7/part-0.parquet\" \n[11] \"month=8/part-0.parquet\"  \"month=9/part-0.parquet\" \n\n\nIn any case, if you want to use the full NYC taxi data, the command you use to open the data set is the same: you just point R to the folder containing the full data set rather than the tiny one.\n\nnyc_taxi <- open_dataset(\"~/Datasets/nyc-taxi/\")\n\nRegardless of which version of the data you’re using, you’re good to go.\n\n\nAncillary data files\nBefore moving on, it’s worth mentioning that the NYC taxi data website also includes a taxi_zone_lookup.csv file that includes human-readable names for the taxi zones. We’ll be using that file in the workshop, so you may wish to download iit also. Here’s a quick look at its contents:\n\nzones <- read_csv_arrow(\"data/taxi_zone_lookup.csv\")\nzones\n\n# A tibble: 265 × 4\n   LocationID Borough       Zone                    service_zone\n        <int> <chr>         <chr>                   <chr>       \n 1          1 EWR           Newark Airport          EWR         \n 2          2 Queens        Jamaica Bay             Boro Zone   \n 3          3 Bronx         Allerton/Pelham Gardens Boro Zone   \n 4          4 Manhattan     Alphabet City           Yellow Zone \n 5          5 Staten Island Arden Heights           Boro Zone   \n 6          6 Staten Island Arrochar/Fort Wadsworth Boro Zone   \n 7          7 Queens        Astoria                 Boro Zone   \n 8          8 Queens        Astoria Park            Boro Zone   \n 9          9 Queens        Auburndale              Boro Zone   \n10         10 Queens        Baisley Park            Boro Zone   \n# … with 255 more rows\n\n\nFinally, it’s also worth mentioning that the website has a shapefile specifying the boundaries of the taxi zones and a csv file with some additional information about them. The tutorial doesn’t use this spatial data (except very briefly at the very end) so you ton’t really need it, but it’s helpful to mention here because it’s a hany way to make sense of the geography of the taxi zones. Using the shapefile together with the sf and ggplot2 packages we can draw a map of the taxi zones coloured by their numeric identifier:\n\nshapefile <- \"data/taxi_zones/taxi_zones.shp\"\nshapedata <- read_sf(shapefile)\n\nshapedata |> \n  ggplot(aes(fill = LocationID)) + \n  geom_sf(size = .1) + \n  theme_bw() + \n  theme(panel.grid = element_blank())\n\n\n\n\nIt’s not really needed, but if you do want a copy of this shapefile, download and unzip the taxi_zones.zip archive file."
  },
  {
    "objectID": "packages-and-data.html#checks",
    "href": "packages-and-data.html#checks",
    "title": "Packages and Data",
    "section": "Checks",
    "text": "Checks\n\nLoading the data\nHere’s what you should expect to see when you print the nyc_taxi object:\n\nnyc_taxi\n\nFileSystemDataset with 158 Parquet files\nvendor_name: string\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\ntrip_distance: double\npickup_longitude: double\npickup_latitude: double\nrate_code: string\nstore_and_fwd: string\ndropoff_longitude: double\ndropoff_latitude: double\npayment_type: string\nfare_amount: double\nextra: double\nmta_tax: double\ntip_amount: double\ntolls_amount: double\ntotal_amount: double\nimprovement_surcharge: double\ncongestion_surcharge: double\npickup_location_id: int64\ndropoff_location_id: int64\nyear: int32\nmonth: int32\n\n\nOkay, so we’re nyc_taxi linked to 158 files. What does that mean in terms of number of rows? How big is the table?\n\nnrow(nyc_taxi)\n\n[1] 1672590319\n\n\nAbout 1.7 billion rows. That’s quite a lot.\n\n\nA dplyr pipeline\nIf you want to make sure everything is working properly, let’s do a simple analysis. We’ll count the number of trips with destinations in each zone:\n\nzone_counts <- nyc_taxi |> \n  count(dropoff_location_id) |> \n  arrange(desc(n)) |> \n  collect() \n\nzone_counts\n\n# A tibble: 265 × 2\n   dropoff_location_id          n\n                 <int>      <int>\n 1                  NA 1249152360\n 2                 236   16489001\n 3                 161   15643391\n 4                 237   15526930\n 5                 170   13328885\n 6                 162   12631075\n 7                 230   12565352\n 8                  48   11482627\n 9                 234   11276120\n10                 142   11136863\n# … with 255 more rows\n\n\nIn order for this to calculation to work, both arrow and dplyr need to be installed and loaded, so this pipeline should work as a good test to see if your installation is working properly!\n\n\nExtras!\nA few other things just for interests sake. If you’re curious to know how long the analysis is expected to take, this is how long it took to run on my laptop. For the sake of simplicity, throughout the workshop we’ll use the tictoc package for timing:\n\ntic() # start timer\nnyc_taxi |> \n  count(dropoff_location_id) |> \n  arrange(desc(n)) |> \n  collect() |> \n  invisible() # suppress printing\ntoc() # stop timer\n\n37.541 sec elapsed\n\n\nTo give a sense of what this result means geographically, because the zones codes aren’t very meaningful without seeing them on a map, here’s that result shown as a heat map:\n\nleft_join(\n  x = shapedata, \n  y = zone_counts, \n  by = c(\"LocationID\" = \"dropoff_location_id\")\n) |> \n  ggplot(aes(fill = n)) + \n  geom_sf(size = .1) + \n  scale_fill_distiller(\n    name = \"Number of trips\",\n    limits = c(0, 17000000), \n    labels = label_comma(),\n    direction = 1\n  ) + \n  theme_bw() + \n  theme(panel.grid = element_blank())\n\n\n\n\nPretty, yes?"
  },
  {
    "objectID": "advanced.html",
    "href": "advanced.html",
    "title": "Part 4: Advanced Arrow",
    "section": "",
    "text": "It is traditional in any technical workshop that by the time you get to the end, two things are happening: the content is moving toward the most complicated material, and the participants are moving toward a state of exhaustion. This collision is… well, it’s a problem. Nobody involved in the workshop – neither the instructors not the participants – really wants to ramp up the technical difficulty or be forced to think too hard. We’re all tired. Which is a little awkward, because the only things left to cover are complicated topics that build on the material from earlier in the workshop. Not only that, every workshop since the dawn of time runs over the time allocated, so we don’t have much time left to cover these topics. It’s a pedagogical knot.\nHere’s how we’re going to untie that knot in this workshop. The final session really is going to talk about advanced topics, but it isn’t going to dive very deep into them, and it’s not going to involve any hands-on exercises. Instead, we’re going to talk about concepts, show some pretty pictures, and point you in the direction of handy resources."
  },
  {
    "objectID": "advanced.html#how-do-we-organize-in-memory-tables",
    "href": "advanced.html#how-do-we-organize-in-memory-tables",
    "title": "Part 4: Advanced Arrow",
    "section": "How do we organize in-memory tables?",
    "text": "How do we organize in-memory tables?\nNow that we’re getting to the end and have a sense of why we’re using Arrow and how we use it, it’s not a bad idea to spend a little time thinking about what in-memory data structures are used by Arrow. How are they organized, and how do they compare to the corresponding structures used by R?\nIn terms of the diagram we’ve been using to structure our view of the arrow package, we’re in the bottom left corner: the focus is on Arrow Tables, R data frames, and all the constituent data structures that comprise these objects.\n\n\n\n\n\nLet’s start by thinking about how it works in R. When we work with tabular data structures in R, we’re almost always using a data frame (or a tibble, which is essentially the same thing) and – as you’re probably well aware given that you’re taking this workshop – a data frame is basically a list of vectors representing columns. In other words, data frames are arranged column-wise in memory. The diagram below shows the data structures involved:\n\n\n\n\n\nNotice that we have only two “tiers” here, vectors and data frames. R doesn’t have any special concept of a “scalar” value: scalars are merely vectors of length 1.\nArrow is a little more complicated. The in-memory tabular data structure you’re most likely to encounter when working with the arrow package is an Arrow Table and as shown below, there are four “tiers”:\n\n\n\n\n\nLet’s unpack these four tiers in a little more detail. At the lowest level we have the concept of a scalar. A scalar object is simply a single value, that can be of any type. It might be an integer, a string, a timestamp, or any of the different data types that Arrow supports (more on that later). For the current purposes, what matters is that a scalar is one value and is considered to be “zero dimensional”. We rarely have a reason to create an Arrow Scalar from R, but for the record, if you ever want to do so you can do this by calling the Scalar$create() function provided by the arrow package:\n\nScalar$create(\"New York\")\n\nScalar\nNew York\n\n\nThe next level up the hierarchy is the array, a fundamental data structure in Arrow. An array is a data structure that stores one or more scalar values, and you probably won’t be surprised to hear that you can create one by calling Array$create():\n\ncity <- Array$create(c(\"New York\", NA, NA, \"Philadelphia\"))\ncity\n\nArray\n<string>\n[\n  \"New York\",\n  null,\n  null,\n  \"Philadelphia\"\n]\n\n\nAn array is a one dimensional stucture, and you can extract subsets by using square brackets:\n\ncity[1:2]\n\nArray\n<string>\n[\n  \"New York\",\n  null\n]\n\n\nSo far this all feels very R like: arrays in Arrow feel very similar to vectors in R at first. However, when we start looking more closely we start seeing differences. An R vector is designed to be an extensible, mutable object. A core design decision in Arrow was to make arrays immutable for performance reasons: under the hood an Arrow array is comprised of a fixed set of buffers – contiguous blocks of memory dedicated to that array – along with some metadata. For example, the city array we just created can be unpacked like this:\n\n\n\n\n\nThe details of this don’t matter too much for this workshop, but we’ll do a lightning tour anyway! The most important things here are the three buffers: the first buffer is a block of memory whose bits indicate which elements (or slots, to use Arrow terminology) of the array valid values and which are null. This buffer is the “validity bitmap”. The second buffer stores numeric values and is referred to as the “offset buffer”: it tells you where to look in the third buffer to find the relevant values to fill each of the scalar strings. This makes most sense when we look at the third “data buffer” which is just one long (utf8-encoded) string containing all the text stored in the array. If we want to extract the text contained in a particular slot in the array we consult the equivalent slot in the offset buffer, which tells us where to start reading text from in the data buffer.\nIn practice, an R user doesn’t need to care about these implementation details, but it’s helpful to understand that these data structures are all design choices that Arrow makes for performance reasons. Making arrays immutable and laying arrays out like this allows Arrow to make extremely efficient use of memory and CPU resources. Unfortunately, this design choice also creates one massive practical problem for data analysts: in real life, data sets change. New data arrive over time, for example. If I cannot add new observations to an array, how can I use an array as the data structure to represent a variable?\nThe solution to the problem is to use chunked arrays. Chunked arrays are a weird kind of fiction intended to bridge the gap between the needs of the data engineer (arrays need to be immutable objects) and the needs of the data analyst (data variables need to be mutable and extensible). The “trick” is quite simple: a chunked array is a list of one or more arrays, coupled with a convenient set of abstractions that allow the user to pretend that these arrays are all laid out as one long vector. This is illustrated schematically below:\n\n\n\n\n\nTo create the chunked array above, I’d do something like this:\n\ncity <- ChunkedArray$create(\n  c(\"New York\", \"San Francisco\", \"Los Angeles\", \"Philadelphia\"),\n  c(\"Sydney\", \"Barcelona\")\n)\n\nOr, alternatively, I could use the convenience function chunked_array() that does exactly the same job:\n\ncity <- chunked_array(\n  c(\"New York\", \"San Francisco\", \"Los Angeles\", \"Philadelphia\"),\n  c(\"Sydney\", \"Barcelona\")\n)\ncity\n\nChunkedArray\n[\n  [\n    \"New York\",\n    \"San Francisco\",\n    \"Los Angeles\",\n    \"Philadelphia\"\n  ],\n  [\n    \"Sydney\",\n    \"Barcelona\"\n  ]\n]\n\n\nNow, in “reality”, what I have done here is create two immutable array objects and wrapped those in a flexible container. Each of those arrays is referred to a “chunk”, and the container a list object that defines the chunked array. But let’s be honest: who cares? The whole point of a chunked array is we can ignore this reality and pretend that this is a length-6 vector. I can extract individual elements using a common indexing scheme:\n\ncity[4:6]\n\nChunkedArray\n[\n  [\n    \"Philadelphia\"\n  ],\n  [\n    \"Sydney\",\n    \"Barcelona\"\n  ]\n]\n\n\nThese results are being pulled from different chunks, but none of that matters at the user level. This abstraction is what allows you to pretend that chunked arrays in Arrow are the same kind of thing as vectors in R.\nAt last we reach the top level of the hierarchy. Recall that our lowest level consists of zero-dimensional objects, scalars. The next two levels, arrays and chunked arrays, are both one-dimensional objects (even if it does require a bit of abstraction to make that work for chunked arrays). Once we reach the table level, however, we are talking about two-dimensional structures.\nIn the same way that an R data frame is an ordered collection of named vectors, an Arrow table is an ordered collection of named chunked arrays. As you might expect given earlier examples, it’s possible to construct a new table using Table$create(), but this time I’ll just jump straight to using the convenience function, arrow_table(). Let’s recreate the riots table exactly, right down to the (completely pointless!) detail of having the exact same chunking structure. To save you scrolling, here’s the table again:\n\n\n\n\n\nHere’s the command:\n\nriots <- arrow_table(\n  location = chunked_array(\n    c(\"Stonewall Inn\", \"Compton's Cafeteria\", \"Cooper Do-nuts\", \"Dewey's\"), \n    c(\"King's Cross\", \"La Rambla\")\n  ), \n  year = chunked_array(\n    c(1969, 1966, 1959, 1965),\n    c(1978, 1977)\n  ),\n  city = chunked_array(\n    c(\"New York\", \"San Francisco\", \"Los Angeles\", \"Philadelphia\"),\n    c(\"Sydney\", \"Barcelona\")\n  )\n)\nriots\n\nTable\n6 rows x 3 columns\n$location <string>\n$year <double>\n$city <string>\n\n\nAdmittedly that is a bit of an anti-climax, because the print method for Arrow tables doesn’t give you a pretty preview, but we can always pull an Arrow table into R to see what it looks like as a tibble. If we have dplyr loaded we can call collect() to do this, but more generally we can coerce the object using as.data.frame():\n\nas.data.frame(riots)\n\n# A tibble: 6 × 3\n  location             year city         \n  <chr>               <dbl> <chr>        \n1 Stonewall Inn        1969 New York     \n2 Compton's Cafeteria  1966 San Francisco\n3 Cooper Do-nuts       1959 Los Angeles  \n4 Dewey's              1965 Philadelphia \n5 King's Cross         1978 Sydney       \n6 La Rambla            1977 Barcelona    \n\n\nThat’s definitely the right data, but this is an object in R and it doesn’t have any analog of “chunks”. If you want to check that the chunking in our original riots table is identical to the one depicted in the illustration, what can do is print out one of the columns:\n\nriots$city\n\nChunkedArray\n[\n  [\n    \"New York\",\n    \"San Francisco\",\n    \"Los Angeles\",\n    \"Philadelphia\"\n  ],\n  [\n    \"Sydney\",\n    \"Barcelona\"\n  ]\n]\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor more information on this topic, see the Arrays and tables in Arrow blog post by Danielle Navarro\n\n\n\nOn record batches\nYou will also encounter the related concept of a record batch. Record batches are fundamental to the design of Apache Arrow, but we don’t typically work with them during everyday data analysis because Tables provide better high-level abstractions for analysis purposes. Nevertheless it is useful to know what they look like. A record batch is a collection of arrays, in the same way that a table is a collection of chunked arrays. Rendered schematically, a record batch is a two-dimensional data structure that looks like this:\n\n\n\n\n\nBecause it is composed of arrays rather than chunked arrays, a record batch is a lot less flexible than a table, so we don’t really use them much for data analysis. However, they are very important data structures when we’re talking about streaming data from one process to another, so when you start digging into the Arrow documentation you’ll find it talks about them a lot. However, for this workshop we’re more interested in Arrow tables (and datasets), so that’s all we’ll say about them here."
  },
  {
    "objectID": "advanced.html#how-are-scalar-types-mapped",
    "href": "advanced.html#how-are-scalar-types-mapped",
    "title": "Part 4: Advanced Arrow",
    "section": "How are scalar types mapped?",
    "text": "How are scalar types mapped?\nIn the last section we saw that there’s a natural(ish) mapping between Arrow Tables and R data frames. They aren’t identical data structures, but it’s conceptually possible to associate one with the other. However, if we want to map from one to another, the mapping needs to decide how to map scalar types from one language to the other. Sometimes this is easy. R has a logical type that can take three values (TRUE, FALSE, NA) and Arrow has an equivalent boolean type that also allows three values (true, false, null). It seems natural to translate R logical vectors to Arrow boolean (chunked) arrays, and vice versa. There are other cases where the data structures are essentially identical: the R integer class and the Arrow int32 type are natural analogs of one another, as are R doubles and Arrow float64 types. These three mappings are shown with bidirectional arrows below:\n\n\n\n\n\nHowever, as you might have guessed by looking at all that blank space, the story becomes a little more complicated once we look at a wider range of types available for representing logicals, integers, and numeric values in R and Arrow. Here’s what the diagram looks like when we fill in the missing pieces:\n\n\n\n\n\nThe dashed lines linked to the uint32, uint64, and int64 types in Arrow exist because the default mappings in those cases are complicated and depend on user settings and on whether the values contained in the variables are representable using the R integer class.\nA similarly nuanced story exists for other data types. There are default bidirectional mappings between character vectors in R and the utf8 type in Arrow, between POSIXct and timestamp, Date and date32, difftime and duration, and finally between the hms::hms class in R and the time32 type in Arrow, but the full story is a little more complex:\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor more information on this topic, see the Data types in Arrow and R blog post by Danielle Navarro"
  },
  {
    "objectID": "advanced.html#the-big-picture",
    "href": "advanced.html#the-big-picture",
    "title": "Part 4: Advanced Arrow",
    "section": "The big picture",
    "text": "The big picture\n\n# ---- the part that really, really needs Arrow ---- \n\nnyc_taxi <- open_dataset(\"~/Datasets/nyc-taxi\")\n\nnyc_taxi_zones <- \"data/taxi_zone_lookup.csv\" |> \n  read_csv_arrow(\n    as_data_frame = FALSE, \n    skip = 1, \n    schema = schema(\n      LocationID = int64(),\n      Borough = utf8(),\n      Zone = utf8(),\n      service_zone = utf8()\n    )\n  ) |>\n  rename(\n    location_id = LocationID,\n    borough = Borough,\n    zone = Zone,\n    service_zone = service_zone\n  )\n  \nairport_zones <- nyc_taxi_zones |>\n  filter(str_detect(zone, \"Airport\")) |>\n  pull(location_id)\n\ndropoff_zones <- nyc_taxi_zones |>\n  select(\n    dropoff_location_id = location_id,\n    dropoff_zone = zone\n  ) \n\nairport_pickups <- nyc_taxi |>\n  filter(pickup_location_id %in% airport_zones) |>\n  select(\n    matches(\"datetime\"),\n    matches(\"location_id\")\n  ) |>\n  left_join(dropoff_zones) |>\n  count(dropoff_zone) |>\n  arrange(desc(n)) |>\n  collect()\n\n# ---- the part that works so perfectly in R ---- \n\ndat <- \"data/taxi_zones/taxi_zones.shp\" |>\n  read_sf() |>\n  clean_names() |>\n  left_join(airport_pickups,\n            by = c(\"zone\" = \"dropoff_zone\")) |>\n  arrange(desc(n))\n\npic <- dat |>\n  ggplot(aes(fill = n)) +\n  geom_sf(size = .1, color = \"#222222\") +\n  scale_fill_distiller(\n    name = \"Number of trips\",\n    labels = label_comma(),\n    palette = \"Oranges\",\n    direction = 1\n  ) +\n  geom_label_repel(\n    stat = \"sf_coordinates\",\n    data = dat |>\n      mutate(zone = case_when(\n        str_detect(zone, \"Airport\") ~ zone,\n        str_detect(zone, \"Times\") ~ zone,\n        TRUE ~ \"\")\n      ),\n    mapping = aes(label = zone, geometry = geometry),\n    max.overlaps = 50,\n    box.padding = .5,\n    label.padding = .5,\n    label.size = .15,\n    label.r = 0,\n    force = 30,\n    force_pull = 0,\n    fill = \"white\",\n    min.segment.length = 0\n  ) +\n  theme_void() +\n  theme(\n    text = element_text(colour = \"black\"), \n    plot.background = element_rect(colour = NA, fill = \"#839496\"),\n    legend.background = element_rect(fill = \"white\"),\n    legend.margin = margin(10, 10, 10, 10)\n  ) +\n  NULL\n\npic"
  },
  {
    "objectID": "data-storage.html",
    "href": "data-storage.html",
    "title": "Part 3: Data Storage",
    "section": "",
    "text": "In this session we’ll talk about reading and writing large data sets. There are a few interrelated topics that arise here, so it’s really helpful to understand that – in terms of the read/write capabilities of the arrow package – we’re focusing almost entirely on the highlighted path in the diagram below:\nWhy this specific pathway?"
  },
  {
    "objectID": "data-storage.html#parquet-files",
    "href": "data-storage.html#parquet-files",
    "title": "Part 3: Data Storage",
    "section": "Parquet files",
    "text": "Parquet files\nIf you work with large data sets already, you may have encountered parquet files before and none of this will be new material for you. But for folks new to this world it’s not as well known, so we’ll talk a little about the Apache Parquet project. Although they are both associated with the Apache Software Foundation, and the arrow package is (as far as we know!) the easiest way to work with parquet files, Apache Parquet is an entirely different project to Apache Arrow.\nParquet files structure a tabular data set in a format that is “row-chunked, column-arranged, and paged”. Here’s what we mean by that. First, we take a table and partition it row-wise into a set of distinct “chunks”, as shown below:\n\n\n\n\n\nThen, for every chunk and for every column in each chunk, that column is split into a collection of “pages”:\n\n\n\n\n\nWhen the table is written to disk, we start writing from chunk 1, column 1, writing each page in order. We then repeat this for every column in this chunk; and then move to the next chunk. This continues until every page has been written:\n\n\n\n\n\nImportantly for our purposes each chunk, column, and page is preceded by relevant metadata. In addition, a metadata block is written at the end of the file containing information about the table, and the locations of the various constituent parts.\nThere are two key features to this format that make it desirable when working with large data sets:\n\nParquet file readers can use metadata to scan the file intelligently: if we know in advance that only some subset of the table is needed for a query, the reader can skip to the relevant pages\nData are stored in a compressed binary format, which reduces file size relative to an uncompressed text format such as CSV.\n\nThat being said, you probably won’t be surprised to hear that we’re glossing over a lot of details here. You wouldn’t be able to code a parquet reader on the basis of this simplified description! However, that’s not our goal here: the purpose of this description is to give you enough of a sense of how a parquet file is organized, so you can understand why they’re handy when working with large data! You can find a lot more information about these details in the Apache Parquet documentation.\nA single parquet file generally contains a quantity of data small enough to load into memory. For example, let’s say I want to load the NYC taxi data for September 2019. This subset of the data can be stored in a 122MB parquet file, and I can load the whole thing into R as a conventional data frame. It’s about 6.5 million rows, but that’s not too much of a challenge:\n\nparquet_file <- \"~/Datasets/nyc-taxi/year=2019/month=9/part-0.parquet\"\n\nnyc_taxi_2019_09 <- read_parquet(parquet_file)\nnyc_taxi_2019_09\n\n# A tibble: 6,567,396 × 22\n   vendor_name pickup_datetime     dropoff_datetime    passenger_count\n   <chr>       <dttm>              <dttm>                        <int>\n 1 VTS         2019-09-01 15:14:09 2019-09-01 15:31:52               2\n 2 VTS         2019-09-01 15:36:17 2019-09-01 16:12:44               1\n 3 VTS         2019-09-01 15:29:19 2019-09-01 15:54:13               1\n 4 CMT         2019-09-01 15:33:09 2019-09-01 15:52:14               2\n 5 VTS         2019-09-01 15:57:43 2019-09-01 16:26:21               1\n 6 CMT         2019-09-01 15:59:16 2019-09-01 16:28:12               1\n 7 CMT         2019-09-01 15:20:06 2019-09-01 15:52:19               1\n 8 CMT         2019-09-01 15:27:54 2019-09-01 15:32:56               0\n 9 CMT         2019-09-01 15:35:08 2019-09-01 15:55:51               0\n10 CMT         2019-09-01 15:19:37 2019-09-01 15:30:52               1\n# … with 6,567,386 more rows, and 18 more variables: trip_distance <dbl>,\n#   pickup_longitude <dbl>, pickup_latitude <dbl>, rate_code <chr>,\n#   store_and_fwd <chr>, dropoff_longitude <dbl>, dropoff_latitude <dbl>,\n#   payment_type <chr>, fare_amount <dbl>, extra <dbl>, mta_tax <dbl>,\n#   tip_amount <dbl>, tolls_amount <dbl>, total_amount <dbl>,\n#   improvement_surcharge <dbl>, congestion_surcharge <dbl>,\n#   pickup_location_id <int>, dropoff_location_id <int>\n\n\nOne thing to highlight here is that the columnar structure to parquet files makes it possible to load only a subset of the columns:\n\nparquet_file |>\n  read_parquet(col_select = matches(\"pickup\"))\n\n# A tibble: 6,567,396 × 4\n   pickup_datetime     pickup_longitude pickup_latitude pickup_location_id\n   <dttm>                         <dbl>           <dbl>              <int>\n 1 2019-09-01 15:14:09               NA              NA                186\n 2 2019-09-01 15:36:17               NA              NA                138\n 3 2019-09-01 15:29:19               NA              NA                132\n 4 2019-09-01 15:33:09               NA              NA                 79\n 5 2019-09-01 15:57:43               NA              NA                132\n 6 2019-09-01 15:59:16               NA              NA                132\n 7 2019-09-01 15:20:06               NA              NA                132\n 8 2019-09-01 15:27:54               NA              NA                224\n 9 2019-09-01 15:35:08               NA              NA                 79\n10 2019-09-01 15:19:37               NA              NA                 97\n# … with 6,567,386 more rows\n\n\nBetter yet, the file reader is faster when only a subset of the columns is needed\n\ntic()\nparquet_file |>\n  read_parquet() |>\n  invisible() # suppress printing\ntoc()\n\n0.867 sec elapsed\n\ntic()\nparquet_file |>\n  read_parquet(col_select = matches(\"pickup\")) |>\n  invisible()\ntoc()\n\n0.165 sec elapsed\n\n\nThis property is handy when dealing with larger-than-memory data: because we can’t load the whole thing into memory, we’re going to have to iteratively read small pieces of the data set. In the next section we’ll talk about how large data sets are typically distributed over many parquet files, but the key thing right now is that whenever we’re loading one of those pieces from a parquet file, an intelligently designed reader will be able to speed things up by reading only the relevant subset each parquet file.\n\n\n\n\n\n\nExercises\n\n\n\n\nProblemsSolution 1Solution 2Solution 3\n\n\n\nLet’s start with some baseline data. Take the data currently stored in nyc_taxi_2019_09 and write it to a CSV file using write_csv_arrow() function supplied by the arrow package. Using the tic() and toc() functions from tictoc package, record how long it took to write the file. Similarly, using the file_size() function from the fs package, see how large the file is.\nRepeat the previous exercise, but this time write the data to a parquet file using write_parquet(). For folks who are new to parquet files: use .parquet as the file extension. How does this compare to the previous exercise?\nTry reading both files into R using read_csv_arrow() and read_parquet(), and compare load times. As a bonus, try the same thing with as_data_frame = FALSE for both files: that way the data will be read into Arrow memory rather than R memory. Is there a difference in elapsed time?\n\n\n\n\ntic()\nwrite_csv_arrow(nyc_taxi_2019_09, \"data/nyc_taxi_2019_09.csv\")\ntoc()\n\n25.553 sec elapsed\n\nfile_size(\"data/nyc_taxi_2019_09.csv\")\n\n940M\n\n\n\n\n\ntic()\nwrite_parquet(nyc_taxi_2019_09, \"data/nyc_taxi_2019_09.parquet\")\ntoc()\n\n2.561 sec elapsed\n\nfile_size(\"data/nyc_taxi_2019_09.parquet\")\n\n125M\n\n\nWriting data from R to a parquet file is faster than writing a CSV file. The end result is much smaller too. The difference in file size is mostly because parquet files are a binary compressed format, whereas CSV files are stored as uncompressed plaintext.\n\n\nRecall that in exercises 1 and 2 I saved the data to \"data/nyc_taxi_2019_09.csv\" and \"data/nyc_taxi_2019_09.parquet\". You may have chosen different file paths!\nThe first part of the problem asks us to read the CSV file and the parquet file into R, and record the time taken:\n\ntic()\n\"data/nyc_taxi_2019_09.csv\" |>\n  read_csv_arrow() |> \n  invisible()\ntoc()\n\n1.603 sec elapsed\n\ntic()\n\"data/nyc_taxi_2019_09.parquet\" |>\n  read_parquet() |> \n    invisible()\ntoc()\n\n0.539 sec elapsed\n\n\nThe parquet file is substantially faster to read.\nThe second part of the problem asks us to repeat the exercise, loading the data as an Arrow Table rather than an R data frame. Here’s how we do that:\n\ntic()\n\"data/nyc_taxi_2019_09.csv\" |>\n  read_csv_arrow(as_data_frame = FALSE) |> \n  invisible()\ntoc()\n\n1.298 sec elapsed\n\ntic()\n\"data/nyc_taxi_2019_09.parquet\" |>\n  read_parquet(as_data_frame = FALSE) |> \n    invisible()\ntoc()\n\n0.557 sec elapsed\n\n\nRead times are fairly similar for Arrow as they are for R. Again, the parquet file is faster than the CSV file."
  },
  {
    "objectID": "data-storage.html#multi-file-data-sets",
    "href": "data-storage.html#multi-file-data-sets",
    "title": "Part 3: Data Storage",
    "section": "Multi-file data sets",
    "text": "Multi-file data sets\nIn our hands-on exercises we’ve been working with the NYC Taxi data, a single tabular data set that is split across 158 distinct parquet files. It’s time to take a closer look at how this works. Let’s start by opening the data:\n\nnyc_taxi <- open_dataset(\"~/Datasets/nyc-taxi/\")\nnyc_taxi\n\nFileSystemDataset with 158 Parquet files\nvendor_name: string\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\n...\n\n\nThe ... in the output indicates truncation: I’m only showing the first few lines of output because it’s the first line that’s important. The nyc_taxi object is an Arrow Dataset represented by 158 files. We can inspect the files field of this object to find the paths to these files:\n\nnyc_taxi$files\n\n  [1] \"/home/danielle/Datasets/nyc-taxi/year=2009/month=1/part-0.parquet\" \n  [2] \"/home/danielle/Datasets/nyc-taxi/year=2009/month=10/part-0.parquet\"\n  [3] \"/home/danielle/Datasets/nyc-taxi/year=2009/month=11/part-0.parquet\"\n  [4] \"/home/danielle/Datasets/nyc-taxi/year=2009/month=12/part-0.parquet\"\n  [5] \"/home/danielle/Datasets/nyc-taxi/year=2009/month=2/part-0.parquet\" \n...\n\n\nNotice that the filenames are structured. They’re organised by year and month and – as you might expect – the data for September 2016 can be found in the year=2016/month=9/ folder. Not only that, the folder names correspond to actual field-value pairings in the data. The nyc_taxi data set has variables named year and month, and those variables can take values 2016 and 9 respectively. This convention, in which folders are named using the relevant filed-value pairs, is referred to as “Hive partitioning”, based on the Apache Hive project.\nPartitioning the data set in a thoughtful way is one of those tricks used to make large data sets manageable. If I want to compute some quantity based only on the rides that took place in September 2019, I can ask the operating system to open the one file containing that data. My query never has to touch the other 157 files. To give a sense of how much of a difference this makes, let’s compare two different queries. The first one extracts a subset of about 10 million rows, based on the partitioning variables:\n\nnyc_taxi |>\n  filter(year == 2016, month == 9) |>\n  nrow()\n\n[1] 10116018\n\n\nThe second one extracts a subset of about the same size, again about 10 million rows, based on the pickup location zone:\n\nnyc_taxi |> \n  filter(pickup_location_id == 138) |> \n  nrow()\n\n[1] 10823589\n\n\nNeither of these queries do very much with the data: they’re just inspecting the metadata to count the number of rows. However, the first query only needs to look at the metadata in one file, whereas the second one has to extract and aggregate data from all 158 files. The difference in compute time is striking:\n\ntic()\nnyc_taxi |>\n  filter(year == 2016, month == 9) |>\n  nrow() |>\n  invisible()\ntoc()\n\n0.026 sec elapsed\n\ntic()\nnyc_taxi |> \n  filter(pickup_location_id == 138) |> \n  nrow() |> \n  invisible()\ntoc()\n\n4.203 sec elapsed\n\n\nAdmittedly, this is a bit of a contrived example, but the core point is still important: partitioning the data set on variables that you’re most likely to query on tends to speed things up.\nThis leads to a natural question: how many variables should we partition on? The nyc_taxi data set is partitioned on year and month, but there’s nothing stopping us from defining a weekday variable that takes on values of Sunday, Monday, etc, and using that to define a third level of partitioning. Or we could have chosen to drop month entirely and partition only on year. Which approach is best?\nThe answer, boringly, is that it depends. As a general rule, if you break the data up into too many small data sets, the operating system has to do too much work searching for the files you want; too few, and you end up with some very large and unwieldy files that are hard to move around and search. So there’s often a sweet spot where you partition based on small number of variables (usually those that are used most often in queries) and end up with a manageable number of files of a manageable size. These rough guidelines can help avoid some known worst cases:\n\nAvoid files smaller than 20MB and larger than 2GB.\nAvoid partitioning layouts with more than 10,000 distinct partitions.\n\nAs an aside, you can apply the same guidelines when thinking about how to structure groups within file types such as parquet that have a notion of row chunks etc, because the same tradeoffs exist there."
  },
  {
    "objectID": "data-storage.html#an-example",
    "href": "data-storage.html#an-example",
    "title": "Part 3: Data Storage",
    "section": "An example",
    "text": "An example\nOkay, enough theory. Let’s actually do this. We’ve already seen the “read” functionality in action, but I’m going to do it again with some additional arguments specified. This time around, I’m going to open a smaller subset, corresponding only to the 2016 data:\n\nnyc_taxi_2016a <- open_dataset(\n  sources = \"~/Datasets/nyc-taxi/year=2016/\",\n  format = \"parquet\",\n  unify_schemas = TRUE\n)\n\nIn this version of the command I’ve explicitly stated that I’m looking for parquet files, though I didn’t really need to do this because format = \"parquet\" is the default. I’ve also set unify_schemas to TRUE rather than the default value of FALSE. What this argument refers to is the way open_dataset() aggregates the data files. When unify_schemas = TRUE, it examines every data file to find names and data types for each column (i.e., the schema for the data set), and then seeks to aggregate those into a coherent whole. This can be time consuming, and is usually unnecessary because the data are written in the exact same format in every file. As a consequence, when unify_schemas = FALSE (the default), the scanner will just look at the first file and assume that every data file has the same schema.\nOkay, so let’s have a look at the data:\n\nnyc_taxi_2016a\n\nFileSystemDataset with 12 Parquet files\nvendor_name: string\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\n...\n\n\nAs expected, this is a multi-file Dataset object constructed from 12 files.\nNext, let’s imagine that we’re about to write an application whose primary function is to look at the different vendors who provide the raw data on a monthly basis. That’s a highly specialized use of this data set, and it may be advantageous to partition by month and vendor_name because those are the variables we’re likely to be querying on later. Because the philosophy of the arrow package is to try to preserve dplyr logic to the greatest possible extent, the default behaviour of write_dataset() is to inspect the grouping variables for the data and use those to construct a Hive-style partition. So if I want to write this Dataset to file using month and vendor as my partitioning variables I would do this:\n\ntic()\nnyc_taxi_2016a |> \n  group_by(month, vendor_name) |>\n  write_dataset(\"~/Datasets/nyc-taxi_2016\")\ntoc()\n\n39.782 sec elapsed\n\n\nAs you can see, this write operation does take a little while to finish, but half a minute isn’t too bad.\nIn any case, I can open the new data set the same way as before:\n\nnyc_taxi_2016b <- open_dataset(\"~/Datasets/nyc-taxi_2016\")\nnyc_taxi_2016b\n\nFileSystemDataset with 27 Parquet files\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\ntrip_distance: double\n...\n\n\nNotice the difference between nyc_taxi_2016a and nyc_taxi_2016b. The both refer to the same data conceptually (i.e., all the taxi rides from 2016), but they’re linked to different files and they carve up the dataset in different ways:\n\nnyc_taxi_2016a$files\n\n [1] \"/home/danielle/Datasets/nyc-taxi/year=2016/month=1/part-0.parquet\" \n [2] \"/home/danielle/Datasets/nyc-taxi/year=2016/month=10/part-0.parquet\"\n [3] \"/home/danielle/Datasets/nyc-taxi/year=2016/month=11/part-0.parquet\"\n [4] \"/home/danielle/Datasets/nyc-taxi/year=2016/month=12/part-0.parquet\"\n [5] \"/home/danielle/Datasets/nyc-taxi/year=2016/month=2/part-0.parquet\" \n...\n\n\n\nnyc_taxi_2016b$files\n\n [1] \"/home/danielle/Datasets/nyc-taxi_2016/month=1/vendor_name=CMT/part-0.parquet\"                       \n [2] \"/home/danielle/Datasets/nyc-taxi_2016/month=1/vendor_name=VTS/part-0.parquet\"                       \n [3] \"/home/danielle/Datasets/nyc-taxi_2016/month=10/vendor_name=CMT/part-0.parquet\"                      \n [4] \"/home/danielle/Datasets/nyc-taxi_2016/month=10/vendor_name=VTS/part-0.parquet\"                      \n [5] \"/home/danielle/Datasets/nyc-taxi_2016/month=11/vendor_name=CMT/part-0.parquet\"                      \n...\n\n\nTo give you a sense of the difference between the two, here’s an example of a (somewhat) realistic query, computed on the nyc_taxi_2016b data:\n\nnyc_taxi_2016b |> \n  filter(vendor_name == \"CMT\") |>\n  group_by(month) |>\n  summarize(distance = sum(trip_distance, na.rm = TRUE)) |>\n  collect()\n\n# A tibble: 12 × 2\n   month  distance\n   <int>     <dbl>\n 1     1 33436823.\n 2    11 13826243.\n 3    12 13642500.\n 4    10 41799496.\n 5     4 27440575.\n 6     2 40006137.\n 7     3 55384892.\n 8     5 52798627.\n 9     6 15617981.\n10     9 76139235.\n11     8 22581320.\n12     7 19210103.\n\n\nHere’s the time taken for this query:\n\n\n0.803 sec elapsed\n\n\nand for the same query performed on the nyc_taxi_2016a data:\n\n\n2.811 sec elapsed\n\n\nThe difference is not quite as extreme as the contrived example earlier, but it’s still quite substantial: using your domain expertise to choose relevant variables to partition on can make a real difference in how your queries perform!\n\n\n\n\n\n\nExercises\n\n\n\n\nProblemsSolution 1Solution 2Solution 3\n\n\n\n(Preliminary) Write a query that picks out the 2019 NYC Taxi data and – in addition to the month and year columns already existing in the data – adds columns for monthday and yearday specifying the day of the month and the day of the year on which the pickup took place (note: lubridate functions day() and yday() are both supported). Check that your query works by selecting the pickup_datetime column and your newly-created monthday and yearday columns and then collecting the first few rows.\nUsing this query, write the 2019 NYC Taxi data to a multi-file dataset – twice. The first time you do it, partition by month and monthday. The second time you do it, partition by yearday. Notice that both of these produce 365 files, each of which contain the exact same subset of data!\nUsing only the datasets that you have just written to disk (i.e., you’ll have to reopen them using open_dataset()), calculate the total amount of money charged (as measured by the total_amount variable) each day, for the 81st through 90th day of the year (using the yearday variable). Do this for both versions of the dataset that you just created, and record how long it took to finish in both cases. What do you notice?\n\n\n\nThe query:\n\nnyc_taxi_2019_days <- nyc_taxi |> \n  filter(year == 2019) |>\n  mutate(\n    monthday = day(pickup_datetime),\n    yearday = yday(pickup_datetime)\n  )\n\nThe check:\n\nnyc_taxi_2019_days |>\n  select(pickup_datetime, monthday, yearday) |>\n  head() |> \n  collect()\n\n# A tibble: 6 × 3\n  pickup_datetime     monthday yearday\n  <dttm>                 <int>   <int>\n1 2019-12-01 12:39:20        1     335\n2 2019-12-01 12:07:55        1     335\n3 2019-12-01 12:22:44        1     335\n4 2019-12-01 12:38:48        1     335\n5 2019-12-01 12:13:21        1     335\n6 2019-12-01 12:39:52        1     335\n\n\n\n\n\ntic()\nnyc_taxi_2019_days |>\n  group_by(month, monthday) |>\n  write_dataset(\"data/nyc_taxi_2019_monthday\")\ntoc()\n\n27.536 sec elapsed\n\ntic()\nnyc_taxi_2019_days |>\n  group_by(yearday) |>\n  write_dataset(\"data/nyc_taxi_2019_yearday\")\ntoc()\n\n27.183 sec elapsed\n\n\n\n\nFirst, we’ll open the two datasets:\n\nnyc_taxi_2019_monthday <- open_dataset(\"data/nyc_taxi_2019_monthday\")\nnyc_taxi_2019_yearday <- open_dataset(\"data/nyc_taxi_2019_yearday\")\n\nHere’s the solution for the month/day version:\n\ntic()\nnyc_taxi_2019_monthday |>\n  filter(yearday %in% 81:90) |>\n  group_by(yearday) |>\n  summarize(gross = sum(total_amount)) |>\n  collect()\n\n# A tibble: 10 × 2\n   yearday    gross\n     <int>    <dbl>\n 1      86 4877420.\n 2      88 4968074.\n 3      87 5115909.\n 4      90 4186767.\n 5      83 4106132.\n 6      82 4578253.\n 7      81 5314584.\n 8      84 4152380.\n 9      85 4611179.\n10      89 4580226.\n\ntoc()\n\n0.347 sec elapsed\n\n\nRepeating the same exercises for the yearday version:\n\ntic()\nnyc_taxi_2019_yearday |>\n  filter(yearday %in% 81:90) |>\n  group_by(yearday) |>\n  summarize(gross = sum(total_amount)) |>\n  collect()\n\n# A tibble: 10 × 2\n   yearday    gross\n     <int>    <dbl>\n 1      83 4106132.\n 2      85 4611179.\n 3      87 5115909.\n 4      88 4968074.\n 5      90 4186767.\n 6      89 4580226.\n 7      81 5314584.\n 8      84 4152380.\n 9      82 4578253.\n10      86 4877420.\n\ntoc()\n\n0.098 sec elapsed\n\n\nThe difference is… not subtle."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Larger-Than-Memory Data Workflows with Apache Arrow",
    "section": "",
    "text": "As datasets become larger and more complex, the boundaries between data engineering and data science are becoming blurred. Data analysis pipelines with larger-than-memory data are becoming commonplace, creating a gap that needs to be bridged: between engineering tools designed to work with very large datasets on the one hand, and data science tools that provide the analysis capabilities used in data workflows on the other. One way to build this bridge is with Apache Arrow, a multi-language toolbox for working with larger-than-memory tabular data. Arrow is designed to improve performance and efficiency, and places emphasis on standardization and interoperability among workflow components, programming languages, and systems. The arrow package provides a mature R interface to Apache Arrow, making it an appealing solution for data scientists working with large data in R.\n\n\n\nIn this tutorial you will learn how to use the arrow R package to create seamless engineering-to-analysis data pipelines. You’ll learn how to use interoperable data file formats like Parquet or Feather for efficient storage and data access. You’ll learn how to exercise fine control over data types to avoid common data pipeline problems. During the tutorial you’ll be processing larger-than-memory files and multi-file datasets with familiar dplyr syntax, and working with data in cloud storage. The tutorial doesn’t assume any previous experience with Apache Arrow: instead, it will provide a foundation for using arrow, giving you access to a powerful suite of tools for analyzing larger-than-memory datasets in R."
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Larger-Than-Memory Data Workflows with Apache Arrow",
    "section": "Instructors",
    "text": "Instructors\n\nDanielle Navarro - Danielle is a data scientist, professional educator, generative artist, former academic in recovery, open source R developer, and author of multiple books on statistics and data analysis.\nJonathan Keane - Jonathan is an engineering and data science manager at Voltron Data. They’ve been passionate about R since undergrad and developed or contributed to a number of open source projects over the years.\nStephanie Hazlitt - Stephanie is a data scientist, an avid R user, and an engineering manager at Voltron Data, with a passion for supporting people and teams in learning, creating and sharing data science products and tools."
  },
  {
    "objectID": "index.html#tutorial-content",
    "href": "index.html#tutorial-content",
    "title": "Larger-Than-Memory Data Workflows with Apache Arrow",
    "section": "Tutorial Content",
    "text": "Tutorial Content\n\n0: Packages and Data. Some instructions on the packages and data sets used in the workshop. It would be handy to read this before the workshop starts!\n1: Hello Arrow. The first session of the workshop provides an overview of the Apache Arrow project and gives participants their first hands on experience working with data using Arrow.\n2: Data Wrangling. The second session is a deep dive into the analyzing large data sets using arrow, dplyr, and to a lesser extent duckdb. This is the longest session of the workshop.\n3: Data Storage. The third session looks in detail the read/write capabilities of arrow. It discusses the parquet file format, how to use it effectively for large data sets, and how to partition large data sets across many files.\n4: Advanced Arrow. The final session is brief, and takes a look under the hood. It talks about the data structures and data types used in Arrow."
  },
  {
    "objectID": "index.html#whenwhere",
    "href": "index.html#whenwhere",
    "title": "Larger-Than-Memory Data Workflows with Apache Arrow",
    "section": "When/Where",
    "text": "When/Where\n\nworkshop_time <- function(tz) {\n  start <- lubridate::ymd_hms(\"2022-06-20 14:00:00\", tz = \"America/Chicago\")\n  close <- lubridate::ymd_hms(\"2022-06-20 17:30:00\", tz = \"America/Chicago\")\n  cat(\"For time zone:\", tz, \"\\n\")\n  cat(\"  start time:\", lubridate::with_tz(start, tz) |> as.character(), \"\\n\")\n  cat(\"  close time:\", lubridate::with_tz(close, tz) |> as.character(), \"\\n\")\n}\n\nworkshop_time(\"America/Vancouver\")\n\nFor time zone: America/Vancouver \n  start time: 2022-06-20 12:00:00 \n  close time: 2022-06-20 15:30:00 \n\nworkshop_time(\"America/Chicago\")\n\nFor time zone: America/Chicago \n  start time: 2022-06-20 14:00:00 \n  close time: 2022-06-20 17:30:00 \n\nworkshop_time(\"Africa/Harare\")\n\nFor time zone: Africa/Harare \n  start time: 2022-06-20 21:00:00 \n  close time: 2022-06-21 00:30:00 \n\nworkshop_time(\"Australia/Sydney\")\n\nFor time zone: Australia/Sydney \n  start time: 2022-06-21 05:00:00 \n  close time: 2022-06-21 08:30:00"
  },
  {
    "objectID": "data-wrangling.html",
    "href": "data-wrangling.html",
    "title": "Part 2: Data Wrangling with Arrow",
    "section": "",
    "text": "In this session we’ll explore the data manipulation capabilities of arrow in a little more detail. The back end that arrow supplies for dplyr has two main jobs: it provides a translation for the dplyr verbs themselves (e.g., mutate(), select(), filter()), and it translates the functions and expressions that are evaluated within those verbs (e.g., year == 2019). In terms of the diagrams we saw in the previous section, we’re focusing on the highlighted part of this dplyr schematic:"
  },
  {
    "objectID": "data-wrangling.html#one-table-computations",
    "href": "data-wrangling.html#one-table-computations",
    "title": "Part 2: Data Wrangling with Arrow",
    "section": "One-table computations",
    "text": "One-table computations\nLet’s start by opening the nyc_taxi dataset:\n\nnyc_taxi <- open_dataset(\"~/Datasets/nyc-taxi/\")\n\n\nExample 1: Basic one-table verbs\nIn the Hello Arrow session we started with an example exercise demonstrating that arrow allows you to use the major one-table verbs in dplyr. To spare you the effort of looking back to the earlier section, here’s the query:\n\nshared_rides <- nyc_taxi |>\n  filter(year %in% 2017:2021) |> \n  group_by(year) |>\n  summarize(\n    all_trips = n(),\n    shared_trips = sum(passenger_count > 1, na.rm = TRUE)\n  ) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) \n\nHere is what happens when we collect() that query:\n\ntic()\ncollect(shared_rides)\n\n# A tibble: 5 × 4\n   year all_trips shared_trips pct_shared\n  <int>     <int>        <int>      <dbl>\n1  2017 113495512     32296166       28.5\n2  2018 102797401     28796633       28.0\n3  2019  84393604     23515989       27.9\n4  2020  24647055      5837960       23.7\n5  2021  30902618      7221844       23.4\n\ntoc()\n\n3.205 sec elapsed\n\n\nNotice that almost all the basic one-table verbs appear in this example. The only one missing is arrange(), and if you did the exercises in the first session you’d have noticed that one of the problems requires you to use that one too. None of the computations are being done in R, but the arrow package provides the back end that translates the dplyr verbs and the expressions contained within to something that the Arrow C++ compute engine understands. It’s magical, but like any code-based magic it helps to recognize it is limited.\n\n\nExample 2: Limitations\nHere’s an example. Suppose I decide those large numbers are annoying and want to express the results of the shared rides computations in millions of rides. It’s easy enough to do this with a single mutate(). First I’ll define a convenient helper function that simply divides by 1 million:\n\nmillions <- function(x) x / 10^6\n\nThen I can apply it separately to both of the trips variables:\n\nshared_rides |>\n  mutate(\n    all_trips = millions(all_trips),\n    shared_trips = millions(shared_trips)\n  ) |>\n  collect()\n\n# A tibble: 5 × 4\n   year all_trips shared_trips pct_shared\n  <int>     <dbl>        <dbl>      <dbl>\n1  2017     113.         32.3        28.5\n2  2018     103.         28.8        28.0\n3  2019      84.4        23.5        27.9\n4  2020      24.6         5.84       23.7\n5  2021      30.9         7.22       23.4\n\n\nWhen there are only a few variables this is easy, but it does get a little cumbersome when there are many so we often use scoped verbs mutate_at() or (in more recent releases of dplyr) use across() to apply a function across a tidy selection of variables. However, at present arrow doesn’t know how to translate this part of the dplyr grammar so both of these fail:\n\nshared_rides |>\n  mutate_at(c(\"all_trips\", \"shared_trips\"), millions) |>\n  collect()\n\nError in if (deparse(expr[[1]]) == name) {: the condition has length > 1\n\nshared_rides |>\n  mutate(across(ends_with(\"trips\"), millions)) |>\n  collect()\n\nError: Expression across(ends_with(\"trips\"), millions) not supported in Arrow\nCall collect() first to pull data into R.\n\n\nBesides resorting to the explicit mutate-each-variable-individually strategy, there are other workarounds. A common one is to recognize that most of the heavy lifting has already been completed in Arrow before ever needing to call across() (or a scoped verb). So it’s often possible to do all the hard work in Arrow, then collect() a small table into R, and then make use of the dplyr tools that weren’t accessible in Arrow. So, for example, both of these work:\n\nshared_rides |>\n  collect() |>\n  mutate_at(c(\"all_trips\", \"shared_trips\"), millions)\n\n# A tibble: 5 × 4\n   year all_trips shared_trips pct_shared\n  <int>     <dbl>        <dbl>      <dbl>\n1  2017     113.         32.3        28.5\n2  2018     103.         28.8        28.0\n3  2019      84.4        23.5        27.9\n4  2020      24.6         5.84       23.7\n5  2021      30.9         7.22       23.4\n\nshared_rides |>\n  collect() |>\n  mutate(across(ends_with(\"trips\"), millions))\n\n# A tibble: 5 × 4\n   year all_trips shared_trips pct_shared\n  <int>     <dbl>        <dbl>      <dbl>\n1  2017     113.         32.3        28.5\n2  2018     103.         28.8        28.0\n3  2019      84.4        23.5        27.9\n4  2020      24.6         5.84       23.7\n5  2021      30.9         7.22       23.4\n\n\n\n\nThe NYC taxi zones table\nOne limitation to the main table in the NYC Taxi data is that pickup and dropoff locations are encoded using a numeric location_id variable that doesn’t give any human-readable interpretation to the geographic regions. There is an auxiliary CSV table provided by NYC TLC that we can use for this purpose. It’s a small table so we don’t need Arrow: we can load the data into R as a tibble. We’ll use the read_csv_arrow() function from the arrow package for this purpose:\n\nnyc_taxi_zones <- \"data/taxi_zone_lookup.csv\" |> \n  read_csv_arrow() |>\n  clean_names()\n\nnyc_taxi_zones\n\n# A tibble: 265 × 4\n   location_id borough       zone                    service_zone\n         <int> <chr>         <chr>                   <chr>       \n 1           1 EWR           Newark Airport          EWR         \n 2           2 Queens        Jamaica Bay             Boro Zone   \n 3           3 Bronx         Allerton/Pelham Gardens Boro Zone   \n 4           4 Manhattan     Alphabet City           Yellow Zone \n 5           5 Staten Island Arden Heights           Boro Zone   \n 6           6 Staten Island Arrochar/Fort Wadsworth Boro Zone   \n 7           7 Queens        Astoria                 Boro Zone   \n 8           8 Queens        Astoria Park            Boro Zone   \n 9           9 Queens        Auburndale              Boro Zone   \n10          10 Queens        Baisley Park            Boro Zone   \n# … with 255 more rows\n\n\nThis is a handy little table: the numeric code in the location_id variable in this table uses the same coding scheme as the pickup_location_id and dropoff_location_id variables in the main NYC Taxi table. The borough column indicates which broad region in the city the zone belongs to (e.g., Manhattan, Queens, Brooklyn), and the zone column provides a human readable name for each taxi zone (e.g., the zone specified by location_id=4 refers to the Alphabet City district). Finally, there’s a service_zone column which isn’t very interesting for our purposes.\nOkay, let’s move this into Arrow:\n\nnyc_taxi_zones_arrow <- arrow_table(nyc_taxi_zones)\nnyc_taxi_zones_arrow\n\nTable\n265 rows x 4 columns\n$location_id <int32>\n$borough <string>\n$zone <string>\n$service_zone <string>\n\n\nTo keep track of what just happened here, notice that we first read the data from disk into R memory, then cleaned the column names while still in R, and then created a copy of the data in Arrow memory:\n\n\n\n\n\nIt’s that last object (the one in Arrow) that we’re going to manipulate, though we’ll write our commands in R with the help of the arrow and dplyr packages.\n\n\nExample 3: String manipulation\nThe nyc_taxi_zones_arrow table is a handy one to look at for two reasons. Firstly it’s small (unlike the main nyc_taxi table!), so you can work with it quickly and let yourself make mistakes without worrying too much about computational efficiency. Secondly, it has text fields that make it a handy way to demonstrate arrow support for regular expressions and string manipulations.\nHere’s an example. Suppose I want to create an abbreviate the names of the taxi zones by removing vowels and spaces (e.g., \"Jamaica Bay\" becomes \"JmcBy\"), and remove anything following a slash (e.g., \"West Chelsea/Hudson Yards\" becomes \"WstChls\"). It’s not the best way of constructing abbreviations but it’ll do for illustrative purposes. For each of these abbreviations I want to calculate the string length, and then sort the results from longest to shortest. If I were writing this for an R data frame like nyc_taxi_zones, here’s the code I’d use:\n\nnyc_taxi_zones |>\n  mutate(\n    abbr_zone = zone |> \n      str_remove_all(\"[aeiou' ]\") |>\n      str_remove_all(\"/.*\"),\n    abbr_zone_len = str_length(abbr_zone)\n  ) |>\n  select(zone, abbr_zone, abbr_zone_len) |>\n  arrange(desc(abbr_zone_len))\n\n# A tibble: 265 × 3\n   zone                         abbr_zone          abbr_zone_len\n   <chr>                        <chr>                      <int>\n 1 Prospect-Lefferts Gardens    Prspct-LffrtsGrdns            18\n 2 Flushing Meadows-Corona Park FlshngMdws-CrnPrk             17\n 3 Springfield Gardens North    SprngfldGrdnsNrth             17\n 4 Springfield Gardens South    SprngfldGrdnsSth              16\n 5 Washington Heights North     WshngtnHghtsNrth              16\n 6 Williamsburg (North Side)    Wllmsbrg(NrthSd)              16\n 7 Financial District North     FnnclDstrctNrth               15\n 8 Washington Heights South     WshngtnHghtsSth               15\n 9 Williamsburg (South Side)    Wllmsbrg(SthSd)               15\n10 Financial District South     FnnclDstrctSth                14\n# … with 255 more rows\n\n\nTo do the same thing for an Arrow Table like nyc_taxi_zones_arrow, I have to make a few modifications. First, as you’d expect given what we covered in the “Hello Arrow!” session, we need end the pipeline with a call to collect(). However I’ve had to modify it in a couple of other ways too:\n\nnyc_taxi_zones_arrow |> \n  mutate(\n    abbr_zone = zone |> \n      str_replace_all(\"[aeiou' ]\", \"\") |>\n      str_replace_all(\"/.*\", \"\")\n  ) |>\n  mutate(\n    abbr_zone_len = str_length(abbr_zone)\n  ) |>\n  select(zone, abbr_zone, abbr_zone_len) |>\n  arrange(desc(abbr_zone_len)) |>\n  collect()\n\n# A tibble: 265 × 3\n   zone                         abbr_zone          abbr_zone_len\n   <chr>                        <chr>                      <int>\n 1 Prospect-Lefferts Gardens    Prspct-LffrtsGrdns            18\n 2 Flushing Meadows-Corona Park FlshngMdws-CrnPrk             17\n 3 Springfield Gardens North    SprngfldGrdnsNrth             17\n 4 Springfield Gardens South    SprngfldGrdnsSth              16\n 5 Washington Heights North     WshngtnHghtsNrth              16\n 6 Williamsburg (North Side)    Wllmsbrg(NrthSd)              16\n 7 Financial District North     FnnclDstrctNrth               15\n 8 Washington Heights South     WshngtnHghtsSth               15\n 9 Williamsburg (South Side)    Wllmsbrg(SthSd)               15\n10 Financial District South     FnnclDstrctSth                14\n# … with 255 more rows\n\n\nThese changes aren’t arbitrary. Translations are never perfect, and you can see hints of that in this example. The exercises below explore this!\n\n\n\n\n\n\nExercises\n\n\n\n\nProblemsSolution 1Solution 2Solution 3\n\n\n\nNotice that there are two separate calls to mutate(): the first one creates the abbr_zone column from the zone column, and the second one creates the abbr_zone_len column from the newly-added abbr_zone column. Natively, it would be possible to collapse these into a single mutate(). What happens when you try that here?\nThe arrow back end understands many R expressions that are not part of dplyr. A lot of those are base R expressions (e.g., trigonometric functions, arithmetic operations, etc) but it also includes many commonly used functions from other packages like stringr and lubridate. This example used str_replace_all() from the stringr package, but if you look closely at the code you might wonder why I didn’t use str_remove_all() instead. Try it and find out why.\nIn the main example I read the data into R before moving it to Arrow. The read_csv_arrow() function allows you to read the data directly from a CSV file to an Arrow table, by setting as_data_frame = FALSE. See if you can recreate the entire pipeline without ever loading the data into an R data frame. Hint: you may need to use dplyr::rename() instead of janitor::clean_names().\n\n\n\n\nnyc_taxi_zones_arrow |> \n  mutate(\n    abbr_zone = zone |> \n      str_replace_all(\"[aeiou' ]\", \"\") |>\n      str_replace_all(\"/.*\", \"\"),\n    abbr_zone_len = str_length(abbr_zone)\n  ) |>\n  select(zone, abbr_zone, abbr_zone_len) |>\n  arrange(desc(abbr_zone_len)) |>\n  collect()\n\n# A tibble: 265 × 3\n   zone                         abbr_zone          abbr_zone_len\n   <chr>                        <chr>                      <int>\n 1 Prospect-Lefferts Gardens    Prspct-LffrtsGrdns            18\n 2 Flushing Meadows-Corona Park FlshngMdws-CrnPrk             17\n 3 Springfield Gardens North    SprngfldGrdnsNrth             17\n 4 Springfield Gardens South    SprngfldGrdnsSth              16\n 5 Washington Heights North     WshngtnHghtsNrth              16\n 6 Williamsburg (North Side)    Wllmsbrg(NrthSd)              16\n 7 Financial District North     FnnclDstrctNrth               15\n 8 Washington Heights South     WshngtnHghtsSth               15\n 9 Williamsburg (South Side)    Wllmsbrg(SthSd)               15\n10 Financial District South     FnnclDstrctSth                14\n# … with 255 more rows\n\n\n\n\n\nnyc_taxi_zones_arrow |> \n  mutate(\n    abbr_zone = zone |> \n      str_remove_all(\"[aeiou' ]\") |>\n      str_remove_all(\"/.*\")\n  ) |>\n  mutate(\n    abbr_zone_len = str_length(abbr_zone)\n  ) |>\n  select(zone, abbr_zone, abbr_zone_len) |>\n  arrange(desc(abbr_zone_len)) |>\n  collect()\n\nWarning: Expression str_remove_all(str_remove_all(zone, \"[aeiou' ]\"), \"/.*\") not\nsupported in Arrow; pulling data into R\n\n\n# A tibble: 265 × 3\n   zone                         abbr_zone          abbr_zone_len\n   <chr>                        <chr>                      <int>\n 1 Prospect-Lefferts Gardens    Prspct-LffrtsGrdns            18\n 2 Flushing Meadows-Corona Park FlshngMdws-CrnPrk             17\n 3 Springfield Gardens North    SprngfldGrdnsNrth             17\n 4 Springfield Gardens South    SprngfldGrdnsSth              16\n 5 Washington Heights North     WshngtnHghtsNrth              16\n 6 Williamsburg (North Side)    Wllmsbrg(NrthSd)              16\n 7 Financial District North     FnnclDstrctNrth               15\n 8 Washington Heights South     WshngtnHghtsSth               15\n 9 Williamsburg (South Side)    Wllmsbrg(SthSd)               15\n10 Financial District South     FnnclDstrctSth                14\n# … with 255 more rows\n\n\n\n\n\n\"data/taxi_zone_lookup.csv\" |> \n  read_csv_arrow(as_data_frame = FALSE) |>\n  rename(\n    location_id = LocationID,\n    borough = Borough,\n    zone = Zone,\n    service_zone = service_zone\n  ) |>  \n  mutate(\n    abbr_zone = zone |> \n      str_replace_all(\"[aeiou' ]\", \"\") |>\n      str_replace_all(\"/.*\", \"\")\n  ) |>\n  mutate(\n    abbr_zone_len = str_length(abbr_zone)\n  ) |>\n  select(zone, abbr_zone, abbr_zone_len) |>\n  arrange(desc(abbr_zone_len)) |>\n  collect()\n\n# A tibble: 265 × 3\n   zone                         abbr_zone          abbr_zone_len\n   <chr>                        <chr>                      <int>\n 1 Prospect-Lefferts Gardens    Prspct-LffrtsGrdns            18\n 2 Flushing Meadows-Corona Park FlshngMdws-CrnPrk             17\n 3 Springfield Gardens North    SprngfldGrdnsNrth             17\n 4 Springfield Gardens South    SprngfldGrdnsSth              16\n 5 Washington Heights North     WshngtnHghtsNrth              16\n 6 Williamsburg (North Side)    Wllmsbrg(NrthSd)              16\n 7 Financial District North     FnnclDstrctNrth               15\n 8 Washington Heights South     WshngtnHghtsSth               15\n 9 Williamsburg (South Side)    Wllmsbrg(SthSd)               15\n10 Financial District South     FnnclDstrctSth                14\n# … with 255 more rows\n\n\nHere’s a depiction of where the data goes in this pipeline:\n\n\n\n\n\nThe only time that any part of the data set (other than metadata) arrives in R is at the very end, where the final result (containing only 265 rows and 3 columns) is pulled into R using collect().\n\n\n\n\n\n\n\n\n\n\n\nHandling unknown expressions\n\n\n\nThe exercises in the previous section show cases when arrow encounters an R expression that it doesn’t know how to translate for the Arrow C++ compute engine to handle. When this occurs, one of two things will happen. If the input object was an in-memory Arrow Table (e.g., nyc_taxi_zones_arrow), it throws a warning, pulls the data back into R, and attempts to complete execution. However, if the input object is an on-disk/load-as-needed Arrow Dataset (e.g., nyc_taxi), the behaviour is more conservative: arrow immediately throws an error.\n\n\n\n\nExample 4: Date/time support\nThe previous example helps give you a sense of where things stand with string manipulation in arrow: most core operations are supported, but you’ll find cases where some of the convenience functions are missing.\nThe current situation (in Arrow 8.0.0) with date/time operations is a little more patchy. Arrow comes with a variety of data types that are well suited to representing dates and times, but the compute engine doesn’t support the full scope of what you can do with a package like lubridate. For example, extractor functions such as year(), month(), day(), and wday() are all available:\n\nnyc_taxi |>\n  filter(\n    year == 2022, \n    month == 1\n  ) |>\n  mutate(\n    day = day(pickup_datetime),\n    weekday = wday(pickup_datetime, label = TRUE),\n    hour = hour(pickup_datetime),\n    minute = minute(pickup_datetime),\n    second = second(pickup_datetime)\n  ) |> \n  filter(\n    hour == 3,\n    minute == 14,\n    second == 15\n  ) |>\n  select(\n    pickup_datetime, year, month, day, weekday\n  ) |> \n  collect()\n\n# A tibble: 5 × 5\n  pickup_datetime      year month   day weekday\n  <dttm>              <int> <int> <int> <chr>  \n1 2022-01-01 14:14:15  2022     1     1 Sat    \n2 2022-01-01 14:14:15  2022     1     1 Sat    \n3 2022-01-01 14:14:15  2022     1     1 Sat    \n4 2022-01-09 14:14:15  2022     1     9 Sun    \n5 2022-01-18 14:14:15  2022     1    18 Tue    \n\n\nNotice the output here is tricky with regard to the hours. The data are in nyc_taxi are stored in UTC, and hours() extracts the hours component of that UTC time. However, I am currently in Sydney. My local time is set to UTC+11, and the print method for the collected R object shows the output in my local time. This is not ideal, but it’s not a failure of arrow per se, just a really annoying mismatch of conventions.\nI’m loathe to say much more about date/time support in arrow because this is a rapidly moving target. For example, right now arrow does not understand the temporal rounding functions round_date(), floor_date(), and ceil_date(), but the sole reason for that is – literally – that I am writing this workshop rather than finishing the pull request that will add this functionality!"
  },
  {
    "objectID": "data-wrangling.html#two-table-computations",
    "href": "data-wrangling.html#two-table-computations",
    "title": "Part 2: Data Wrangling with Arrow",
    "section": "Two-table computations",
    "text": "Two-table computations\n\nExample 5: Basic joins\nI’ll use the penguins data from the palmerpenguins package for this example. You don’t actually need to know anything about the data set but in case you don’t know it or need a reminder, here it is:\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# … with 334 more rows, and 2 more variables: sex <fct>, year <int>\n\n\nThe penguins data set tells you the name of the island but doesn’t include any spatial information about where these islands are. So we could imagine having a small lookup table called location that contains an approximate longitude and latitude for each island (we’re being lazy and representing each island as an idealized point!)\n\nlocation <- arrow_table(\n  island = c(\"Torgersen\", \"Biscoe\", \"Dream\"), \n  lon = c(-64.77, -65.43, -64.73),\n  lat = c(-64.08, -65.50, -64.23)\n)  \nlocation\n\nTable\n3 rows x 3 columns\n$island <string>\n$lon <double>\n$lat <double>\n\n\nI’ve used arrow_table() to create this lookup table directly in Arrow because in a moment I’m going to want to send the penguins data over to Arrow also, and then left join the location table onto the penguins table so that we now have columns for lon and lat. I imagine everyone taking this workshop is familiar with data base joins and with the dplyr syntax for performing them, but let’s make this super easy and include a pretty picture illustrating what left_join() is going to do for us:\n\n\n\n\n\nOf course the real penguins data frame has more rows and columns than the ones shown, but that’s not really the point!\nSo let’s do this. Our initial dplyr pipe looks like this: we convert penguins to an Arrow Table using the very evocatively named arrow_table() function, and then left_join() that table with the location table (which is already in Arrow):\n\npenguins |> \n  arrow_table() |>\n  left_join(location)\n\nTable (query)\nspecies: dictionary<values=string, indices=int8>\nisland: dictionary<values=string, indices=int8>\nbill_length_mm: double\nbill_depth_mm: double\nflipper_length_mm: int32\nbody_mass_g: int32\nsex: dictionary<values=string, indices=int8>\nyear: int32\nlon: double\nlat: double\n\nSee $.data for the source Arrow object\n\n\nThe result is an unevaluated query. If we want to execute this and then pull the results back into R we need to call collect() at the end of the pipeline. So we’ll do that, after using select() so that the columns returned just so happen to be the same ones shown in the pretty picture above:\n\npenguins |> \n  arrow_table() |>\n  left_join(location) |> \n  select(species, island, bill_length_mm, lon, lat) |>\n  collect()\n\n# A tibble: 344 × 5\n   species island    bill_length_mm   lon   lat\n   <fct>   <fct>              <dbl> <dbl> <dbl>\n 1 Adelie  Torgersen           39.1 -64.8 -64.1\n 2 Adelie  Torgersen           39.5 -64.8 -64.1\n 3 Adelie  Torgersen           40.3 -64.8 -64.1\n 4 Adelie  Torgersen           NA   -64.8 -64.1\n 5 Adelie  Torgersen           36.7 -64.8 -64.1\n 6 Adelie  Torgersen           39.3 -64.8 -64.1\n 7 Adelie  Torgersen           38.9 -64.8 -64.1\n 8 Adelie  Torgersen           39.2 -64.8 -64.1\n 9 Adelie  Torgersen           34.1 -64.8 -64.1\n10 Adelie  Torgersen           42   -64.8 -64.1\n# … with 334 more rows\n\n\nThe example is very simple and only considers one type of join, but the point is simply to illustrate that joins are supported and the syntax is more or less what you’d expect.\n\n\nExample 6: Traps for the unwary\nNow let’s do something more a little more realistic using the nyc_taxi data, but in doing so we’re going to need to be very careful.\nLet’s imagine that our task is to analyze the taxi data as a function of the pickup and dropoff boroughs. To start with we’ll keep things simple: we’ll ignore the dropoff location and focus only on the pickups. All we want to do is add a column to the nyc_taxi dataset named pickup_borough. We’ll go through this slowly. First, let’s create a version of the taxi zones data that contains appropriately named variables:\n\npickup <- nyc_taxi_zones |> \n  select(\n    pickup_location_id = location_id, \n    pickup_borough = borough\n  )\n\npickup\n\n# A tibble: 265 × 2\n   pickup_location_id pickup_borough\n                <int> <chr>         \n 1                  1 EWR           \n 2                  2 Queens        \n 3                  3 Bronx         \n 4                  4 Manhattan     \n 5                  5 Staten Island \n 6                  6 Staten Island \n 7                  7 Queens        \n 8                  8 Queens        \n 9                  9 Queens        \n10                 10 Queens        \n# … with 255 more rows\n\n\nOkay, so now we have (what appears to be) the perfect table to left join on. The nyc_taxi and pickup tables both have an integer-valued pickup_location_id column that we can use as the join key, and the pickup_borough column is named appropriately for insertion into nyc_taxi. Because the left_join() function in dplyr will automatically join on columns with the same name, the join command looks like this:\n\nnyc_taxi |> \n  left_join(pickup)\n\nOn the surface this query looks like it should work: we know that it’s well-formed dplyr code, and we know from the previous example that arrow supports database joins, so it should work, right? What we’re expecting to see is a left join exactly analogous to our penguins example:\n\n\n\n\n\nInstead we get an error!\n\nnyc_taxi |> \n  left_join(pickup) |>\n  collect()\n\nError in `collect()`:\n! Invalid: Incompatible data types for corresponding join field keys: FieldRef.Name(pickup_location_id) of type int64 and FieldRef.Name(pickup_location_id) of type int32\n\n\nIf we dig into this error message it’s telling us that we’ve encountered a type mismatch. The nyc_taxi and pickup tables both contain a column named pickup_location_id, and so the left_join() function has attempted to join on that column. However, the two columns don’t store the same kind of data, so Arrow throws an error message. That might seem a little odd because they’re both storing integer values, and that’s the same type of data right?\nNot necessarily. You’re probably going to encounter this in the wild so let’s unpack it now.\n\n\nSchemas and data types\nArrow is quite picky about data types. In our previous example, both tables store the pickup_location_id variable as an integer, but they haven’t stored it as the same kind of integer: in the nyc_taxi data it’s encoded as a 64-bit integer, but in the pickup table it’s encoded as a 32-bit integer. Arrow won’t treat these as equivalent, so whenever you’re joining tables you need to pay close attention to data types. Later on in the workshop I’ll talk more about the various data types that exist in Arrow and R and how they’re translated, but for now it’s sufficient to recognise that a 32-bit integer is not the same thing as a 64-bit integer.\nSo how do we diagnose this issue, and how do we fix it?\nBack at the start of the workshop when we first opened the dataset, we saw some output listing all the variables contained in the data and their types. The formal name for this in Arrow is the schema of the data set, and we can check the schema explicitly like this:\n\nnyc_taxi$schema\n\nSchema\nvendor_name: string\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\ntrip_distance: double\npickup_longitude: double\npickup_latitude: double\nrate_code: string\nstore_and_fwd: string\ndropoff_longitude: double\ndropoff_latitude: double\npayment_type: string\nfare_amount: double\nextra: double\nmta_tax: double\ntip_amount: double\ntolls_amount: double\ntotal_amount: double\nimprovement_surcharge: double\ncongestion_surcharge: double\npickup_location_id: int64\ndropoff_location_id: int64\nyear: int32\nmonth: int32\n\n\nNear the bottom of that list you can see that the entry for pickup_location_id is int64.\nOkay, now think about what happens when we try to join on the pickup data. The pickup data is currently an R data frame but when the join takes place it gets implicitly converted to an Arrow table, and this implicit conversion relies on some default assumptions about what kind of data types should be created in Arrow. So let’s have a look at the schema for the object that gets created:\n\narrow_table(pickup)$schema\n\nSchema\npickup_location_id: int32\npickup_borough: string\n\n\nThere’s our type mismatch. In the schema for this table, pickup_location_id is int32.\nSo how do we fix it? There are several ways you can do this, but the simplest is probably to explicitly define the schema for the auxiliary table. Let’s move the nyc_taxi_zones data from R to Arrow using the as_arrow_table() command:\n\nnyc_taxi_zones <- nyc_taxi_zones |> \n  as_arrow_table(\n    schema = schema(\n      location_id = int64(),\n      borough = utf8(), \n      zone = utf8(),\n      service_zone = utf8()\n    )\n  )\n\nnyc_taxi_zones$schema\n\nSchema\nlocation_id: int64\nborough: string\nzone: string\nservice_zone: string\n\n\nNow that the location_id column in nyc_taxi_zones is an Arrow int64 type, our subsequent operations in defining a pickup table (as well as the dropoff table that I’m about to use) will inherit this property. The type mismatch problem will go way and we can now write a join that will “just work”.\n\n\nExample 6 revisited: Fixing the join\nOnce we have a properly formatted auxiliary table, our task becomes much simpler. Let’s join twice, once to insert a pickup_borough column into the NYC Taxi data, and a second time to insert a dropoff_borough into the data. Once this is done we can compute cross-tabulations: for every combination of pickup and dropoff borough, how many rides are there that start and end in the respective regions of the city?\n\npickup <- nyc_taxi_zones |> \n  select(\n    pickup_location_id = location_id, \n    pickup_borough = borough\n  )\n\ndropoff <- nyc_taxi_zones |> \n  select(\n    dropoff_location_id = location_id, \n    dropoff_borough = borough\n  )\n\ntic()\nborough_counts <- nyc_taxi |> \n  left_join(pickup) |>\n  left_join(dropoff) |>\n  count(pickup_borough, dropoff_borough) |>\n  arrange(desc(n)) |>\n  collect()\ntoc()\n\n525.445 sec elapsed\n\n\nBe warned! Although the code is clean and it all works nicely, this query does take a little while to complete. Still, when you consider the fact that a humble 2022 Dell XPS13 laptop just joined twice onto a 1.7 billion row table and cross-tabulated by the newly-inserted columns, that’s not too shabby. Anyway, here’s the results:\n\nborough_counts\n\n# A tibble: 50 × 3\n   pickup_borough dropoff_borough          n\n   <chr>          <chr>                <int>\n 1 <NA>           <NA>            1249152360\n 2 Manhattan      Manhattan        355850092\n 3 Queens         Manhattan         14648891\n 4 Manhattan      Queens            13186443\n 5 Manhattan      Brooklyn          11294128\n 6 Queens         Queens             7537042\n 7 Unknown        Unknown            4519763\n 8 Queens         Brooklyn           3727686\n 9 Brooklyn       Brooklyn           3566671\n10 Manhattan      Bronx              2091627\n# … with 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\nProblemsSolution 1\n\n\n\nHow many taxi pickups were recorded in 2019 from the three major airports covered by the NYC Taxis data set (JFK, LaGuardia, Newark)?\n\n\n\n\npickup_location <- nyc_taxi_zones |> \n  select(\n    pickup_location_id = location_id, \n    pickup_zone = zone\n  )\n\ntic()\nnyc_taxi |> \n  filter(year == 2019) |>\n  left_join(pickup_location) |>\n  filter(str_detect(pickup_zone, \"Airport\")) |>\n  count(pickup_zone) |>\n  collect()\n\n# A tibble: 3 × 2\n  pickup_zone             n\n  <chr>               <int>\n1 LaGuardia Airport 2159224\n2 JFK Airport       2729336\n3 Newark Airport       8643\n\ntoc()\n\n17.117 sec elapsed\n\n\nThe result here seemed slightly surprising to me initially (not being from the US, much less New York), but it makes a little more when you look at the geography: Newark airport is in New Jersey, and not strictly speaking a part of New York City. This has an impact on how the zones are categorized, what fees are charged, and (I presume) on whether the taxis that service the airport tend to be captured by the NYC TLC data. It’s not very relevant to our goals in this workshop but of course would be important in a real world data analysis :)"
  },
  {
    "objectID": "data-wrangling.html#combining-arrow-with-duckdb",
    "href": "data-wrangling.html#combining-arrow-with-duckdb",
    "title": "Part 2: Data Wrangling with Arrow",
    "section": "Combining arrow with duckdb",
    "text": "Combining arrow with duckdb\nAt the start of the workshop we emphasized the point that Arrow aims to be a multi-language toolbox that makes it easier to connect analytics applications. Here’s an example: DuckDB.\nIf you’re not familiar with it, DuckDB is a lightweight database system designed specifically for analytics. It’s serverless, has no external dependencies, and if you want to use it with R all you have to do is install the duckdb package with install.packages(\"duckdb\"). Think of it as like SQLite, with a major exception: it’s intended to support analytics workflows (fewer queries, but computationally expensive ones) rather than transactional ones (many queries that touch only a small part of the database). For the kind of workflows a data scientist tends to engage in, it’s usually much faster than SQLite. Plus, it comes with Arrow support, and consequently the arrow and duckdb packages play very nicely together.\nHere’s how it works: First, notice DuckDB is an SQL database, and you can interact with it from R using dbplyr. Second, notice that DuckDB understands Arrow formats, so we can pass control of a dataset from the Arrow C++ library to DuckDB simply by passing pointers: there’s no copying involved. These two things together are what allow the arrow package to supply two very handy functions, to_duckdb() and to_arrow(). What these do, in effect, is allow you to switch out one engine for the other in the middle of a data analysis pipeline at almost zero cost!\n\n\n\n\n\nThis can be remarkably powerful. There are some operations that are supported by DuckDB but not currently available in the Arrow compute engine, and vice versa. So it’s easy just flip back and forth between them whenever you need to.\n(As an aside, if you’re like me this feels… wrong. Under normal circumstances you’d think it’s absurdly wasteful to completely swap out the back end engine just to get access to a couple of handy functions, but that’s because we’re all so used to a mental model in which serialization costs are pervasive. We’ve all internalized the idea that switching costs are really, really big. But in this case all we’re really doing is passing a little bit of metadata between two processes that both understand Arrow and dplyr so it’s really not a big deal)\n\nExample 7: Window functions\nOkay let’s start with a simple example that intuitively you’d think should work, and I’ll use the penguins data from the palmerpenguins package again. I’m going to add a column specifying the row number, filter out some observations, and select a few columns. Nothing fancy, and if I do it natively in R, everything works fine:\n\npenguins |> \n  mutate(id = row_number()) |>\n  filter(is.na(sex)) |> \n  select(id, sex, species, island)\n\n# A tibble: 11 × 4\n      id sex   species island   \n   <int> <fct> <fct>   <fct>    \n 1     4 <NA>  Adelie  Torgersen\n 2     9 <NA>  Adelie  Torgersen\n 3    10 <NA>  Adelie  Torgersen\n 4    11 <NA>  Adelie  Torgersen\n 5    12 <NA>  Adelie  Torgersen\n 6    48 <NA>  Adelie  Dream    \n 7   179 <NA>  Gentoo  Biscoe   \n 8   219 <NA>  Gentoo  Biscoe   \n 9   257 <NA>  Gentoo  Biscoe   \n10   269 <NA>  Gentoo  Biscoe   \n11   272 <NA>  Gentoo  Biscoe   \n\n\nOkay let’s imagine I’m doing all this in Arrow. I’ll add an arrow_table() step at the top of the pipeline so that all the subsequent steps are being done using an Arrow table not an R data frame, and…\n\npenguins |> \n  arrow_table() |> \n  mutate(id = row_number()) |> # oh no!\n  filter(is.na(sex)) |> \n  select(id, sex, species, island)\n\nWarning: Expression row_number() not supported in Arrow; pulling data into R\n\n\n…it doesn’t work. The row_number() function is not currently understood by the Arrow back end. In fact, this is a special case of a general issue: engine supplied by the Arrow C++ library doesn’t currently support window functions, and as a consequence the arrow R package can’t do much to help. However, the DuckDB engine does support window functions, so all we have to do is pass control over to_duckdb():\n\npenguins |> \n  arrow_table() |> \n  to_duckdb() |> \n  mutate(id = row_number()) |>\n  filter(is.na(sex)) |> \n  select(id, sex, species, island)\n\n# Source:   lazy query [?? x 4]\n# Database: DuckDB 0.3.4 [danielle@Linux 5.13.0-51-generic:R 4.2.0/:memory:]\n      id sex   species island   \n   <dbl> <chr> <chr>   <chr>    \n 1     4 <NA>  Adelie  Torgersen\n 2     9 <NA>  Adelie  Torgersen\n 3    10 <NA>  Adelie  Torgersen\n 4    11 <NA>  Adelie  Torgersen\n 5    12 <NA>  Adelie  Torgersen\n 6    48 <NA>  Adelie  Dream    \n 7   179 <NA>  Gentoo  Biscoe   \n 8   219 <NA>  Gentoo  Biscoe   \n 9   257 <NA>  Gentoo  Biscoe   \n10   269 <NA>  Gentoo  Biscoe   \n# … with more rows\n\n\nIt all works now.\n\n\nExample 8: Something bigger!\nThe example you’ve just seen highlights the basic idea but it’s tiny: a 344 row data set doesn’t really need Arrow at all. So let’s scale this up a little! Let’s suppose I’m working with the NYC taxi data for January 2022 and I’ve opened it as an Arrow dataset:\n\nnyc_taxi_jan <- open_dataset(\"~/Datasets/nyc-taxi/year=2022/month=1/\")\n\nIt’s small relative to the full taxi data, but it’s still two and a half million rows. Large enough that to potentially cause headaches if we aren’t cautious:\n\nnyc_taxi_jan$num_rows \n\n[1] 2463879\n\n\nNow let’s give ourselves a slightly weird “numerological” task: we want to rank the taxi rides in chronological order by pickup time. We then want to find those taxi rides where the pickup occurred on the 59th minute of the hour, and the 59th second of minute, and the numerical rank of that ride contains the number 59 in it. We would like to return the rank, the pickup time, and a “magic number” that removes all the digits from the rank that are not 5 or 9. This would be a little tricky with arrow and dplyr alone, but not too difficult with duckdb and dbplyr:\n\ntic()\nnumerology <- nyc_taxi_jan |>\n  to_duckdb() |>  \n  window_order(pickup_datetime) |>\n  mutate(trip_id = row_number()) |>\n  filter(\n    trip_id |> as.character() |> str_detect(\"59\"),\n    second(pickup_datetime) == 59,\n    minute(pickup_datetime) == 59\n  ) |> \n  mutate(\n    magic_number = trip_id |> \n      as.character() |> \n      str_remove_all(\"[^59]\") |>\n      as.integer()\n  ) |>\n  select(trip_id, magic_number, pickup_datetime) |>\n  collect()\ntoc()\n\n0.72 sec elapsed\n\nnumerology\n\n# A tibble: 37 × 3\n   trip_id magic_number pickup_datetime    \n     <dbl>        <int> <dttm>             \n 1   13159           59 2022-01-01 02:59:59\n 2   39159          959 2022-01-01 15:59:59\n 3   65940           59 2022-01-02 01:59:59\n 4  159315          595 2022-01-03 14:59:59\n 5  405954          595 2022-01-06 18:59:59\n 6  405955         5955 2022-01-06 18:59:59\n 7  487592           59 2022-01-07 21:59:59\n 8  590708           59 2022-01-09 07:59:59\n 9  590709          599 2022-01-09 07:59:59\n10  590710           59 2022-01-09 07:59:59\n# … with 27 more rows\n\n\n\n\n\n\n\n\nResources on Arrow/DuckDB integration\n\n\n\nIf you’re interested in following up on this topic, a good place to start is with the Bigger data with arrow and duckdb slides prepared by Thomas Mock and Edgar Ruiz"
  }
]